{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed5f46-4571-4de0-a150-2351b41449f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CICIoT2023 Dataset ML Algorithm with GOA and Hybrid RF-XGB\n",
      "This implementation includes:\n",
      "- Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\n",
      "- Hybrid Random Forest + XGBoost ensemble model\n",
      "- Three class representations: 2-class, 8-class, and 34-class\n",
      "\n",
      "To use this algorithm:\n",
      "1. Ensure you have the CICIoT2023 dataset in CSV format\n",
      "2. Install required packages: pip install pandas scikit-learn imbalanced-learn xgboost\n",
      "3. Run for different representations:\n",
      "   - 2-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '2-class')\n",
      "   - 8-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '8-class')\n",
      "   - 34-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '34-class')\n",
      "Starting CICIoT2023 ML Algorithm with 2-class representation...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (253575, 47)\n",
      "Columns: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "Target column: label\n",
      "Target distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Handling missing values and duplicates...\n",
      "Removed 0 duplicate rows\n",
      "Final dataset shape after cleaning: (253575, 47)\n",
      "Handling outliers...\n",
      "Creating 2-class representation...\n",
      "Unique classes in dataset: ['DDoS-ICMP_Flood' 'DDoS-PSHACK_Flood' 'DoS-UDP_Flood' 'DDoS-SYN_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'DDoS-RSTFINFlood' 'Mirai-greip_flood'\n",
      " 'DDoS-SynonymousIP_Flood' 'Mirai-greeth_flood' 'DDoS-TCP_Flood'\n",
      " 'DDoS-UDP_Flood' 'DDoS-ICMP_Fragmentation' 'DoS-TCP_Flood' 'Recon-OSScan'\n",
      " 'DDoS-UDP_Fragmentation' 'MITM-ArpSpoofing' 'Mirai-udpplain'\n",
      " 'DoS-SYN_Flood' 'BenignTraffic' 'Recon-HostDiscovery' 'DDoS-SlowLoris'\n",
      " 'Recon-PingSweep' 'DNS_Spoofing' 'DoS-HTTP_Flood' 'DDoS-HTTP_Flood'\n",
      " 'VulnerabilityScan' 'DictionaryBruteForce' 'Recon-PortScan'\n",
      " 'SqlInjection' 'CommandInjection' 'XSS' 'BrowserHijacking'\n",
      " 'Uploading_Attack' 'Backdoor_Malware']\n",
      "Class counts: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "No explicit 'Normal' class found. Using 'BenignTraffic' as normal class.\n",
      "Normal samples: 5882, Attack samples: 247693\n",
      "Warning: Insufficient samples. Using 5882 samples per class instead of 8450\n",
      "Final 2-class dataset shape: (11764, 47)\n",
      "Class distribution:\n",
      "label\n",
      "Normal    5882\n",
      "Attack    5882\n",
      "Name: count, dtype: int64\n",
      "Training set size: 7528\n",
      "Validation set size: 1883\n",
      "Test set size: 2353\n",
      "Normalizing features...\n",
      "Training hybrid Random Forest + XGBoost model with GOA optimization...\n",
      "Starting Gazelle Optimization Algorithm...\n",
      "GOA Iteration 1/30, Best Fitness: 0.0085\n",
      "GOA Iteration 2/30, Best Fitness: 0.0085\n",
      "GOA Iteration 3/30, Best Fitness: 0.0085\n",
      "GOA Iteration 4/30, Best Fitness: 0.0080\n",
      "GOA Iteration 5/30, Best Fitness: 0.0080\n",
      "GOA Iteration 6/30, Best Fitness: 0.0080\n",
      "GOA Iteration 7/30, Best Fitness: 0.0080\n",
      "GOA Iteration 8/30, Best Fitness: 0.0080\n",
      "GOA Iteration 9/30, Best Fitness: 0.0080\n",
      "GOA Iteration 10/30, Best Fitness: 0.0080\n",
      "GOA Iteration 11/30, Best Fitness: 0.0080\n",
      "GOA Iteration 12/30, Best Fitness: 0.0080\n",
      "GOA Iteration 13/30, Best Fitness: 0.0080\n",
      "GOA Iteration 14/30, Best Fitness: 0.0080\n",
      "GOA Iteration 15/30, Best Fitness: 0.0080\n",
      "GOA Iteration 16/30, Best Fitness: 0.0080\n",
      "GOA Iteration 17/30, Best Fitness: 0.0080\n",
      "GOA Iteration 18/30, Best Fitness: 0.0080\n",
      "GOA Iteration 19/30, Best Fitness: 0.0080\n",
      "GOA Iteration 20/30, Best Fitness: 0.0080\n",
      "GOA Iteration 21/30, Best Fitness: 0.0080\n",
      "GOA Iteration 22/30, Best Fitness: 0.0080\n",
      "GOA Iteration 23/30, Best Fitness: 0.0080\n",
      "GOA Iteration 24/30, Best Fitness: 0.0080\n",
      "GOA Iteration 25/30, Best Fitness: 0.0080\n",
      "GOA Iteration 26/30, Best Fitness: 0.0080\n",
      "GOA Iteration 27/30, Best Fitness: 0.0080\n",
      "GOA Iteration 28/30, Best Fitness: 0.0080\n",
      "GOA Iteration 29/30, Best Fitness: 0.0080\n",
      "GOA Iteration 30/30, Best Fitness: 0.0080\n",
      "GOA optimization completed. Best fitness: 0.0080\n",
      "Best RF parameters: {'n_estimators': 181, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1, 'random_state': 42, 'n_jobs': -1}\n",
      "Best XGB parameters: {'n_estimators': 193, 'max_depth': 3, 'learning_rate': 0.1232868110114114, 'subsample': 0.9809928763016946, 'colsample_bytree': 1.0, 'random_state': 42, 'n_jobs': -1, 'eval_metric': 'logloss'}\n",
      "Ensemble weights - RF: 0.483, XGB: 0.517\n",
      "Hybrid model training completed!\n",
      "Evaluating hybrid model...\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - 2-CLASS REPRESENTATION\n",
      "================================================================================\n",
      "Hybrid Random Forest + XGBoost Model (GOA Optimized):\n",
      "  Accuracy:  0.9915\n",
      "  Precision: 0.9916\n",
      "  Recall:    0.9915\n",
      "  F1-Score:  0.9915\n",
      "\n",
      "Optimized Hyperparameters:\n",
      "  RF n_estimators: 181\n",
      "  RF max_depth: 15\n",
      "  XGB n_estimators: 193\n",
      "  XGB learning_rate: 0.123\n",
      "  Ensemble weights - RF: 0.483, XGB: 0.517\n",
      "Starting CICIoT2023 ML Algorithm with 8-class representation...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (253575, 47)\n",
      "Columns: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "Target column: label\n",
      "Target distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Handling missing values and duplicates...\n",
      "Removed 0 duplicate rows\n",
      "Final dataset shape after cleaning: (253575, 47)\n",
      "Handling outliers...\n",
      "Creating 8-class representation...\n",
      "Unique classes in dataset: ['DDoS-ICMP_Flood' 'DDoS-PSHACK_Flood' 'DoS-UDP_Flood' 'DDoS-SYN_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'DDoS-RSTFINFlood' 'Mirai-greip_flood'\n",
      " 'DDoS-SynonymousIP_Flood' 'Mirai-greeth_flood' 'DDoS-TCP_Flood'\n",
      " 'DDoS-UDP_Flood' 'DDoS-ICMP_Fragmentation' 'DoS-TCP_Flood' 'Recon-OSScan'\n",
      " 'DDoS-UDP_Fragmentation' 'MITM-ArpSpoofing' 'Mirai-udpplain'\n",
      " 'DoS-SYN_Flood' 'BenignTraffic' 'Recon-HostDiscovery' 'DDoS-SlowLoris'\n",
      " 'Recon-PingSweep' 'DNS_Spoofing' 'DoS-HTTP_Flood' 'DDoS-HTTP_Flood'\n",
      " 'VulnerabilityScan' 'DictionaryBruteForce' 'Recon-PortScan'\n",
      " 'SqlInjection' 'CommandInjection' 'XSS' 'BrowserHijacking'\n",
      " 'Uploading_Attack' 'Backdoor_Malware']\n",
      "Class counts: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "8-class categories: ['DDoS' 'DoS' 'Mirai' 'Recon' 'Spoofing' 'BruteForce' 'Web']\n",
      "Target samples per class: 33800\n",
      "Original class distribution: label\n",
      "DDoS          190844\n",
      "DoS            43817\n",
      "Mirai          14221\n",
      "Spoofing        2685\n",
      "Recon           1864\n",
      "Web               73\n",
      "BruteForce        71\n",
      "Name: count, dtype: int64\n",
      "Final 8-class dataset shape: (105428, 47)\n",
      "Class distribution:\n",
      "label\n",
      "DDoS          33800\n",
      "DoS           33800\n",
      "Mirai         28442\n",
      "Spoofing       5370\n",
      "Recon          3728\n",
      "Web             146\n",
      "BruteForce      142\n",
      "Name: count, dtype: int64\n",
      "Training set size: 67473\n",
      "Validation set size: 16869\n",
      "Test set size: 21086\n",
      "Normalizing features...\n",
      "Training hybrid Random Forest + XGBoost model with GOA optimization...\n",
      "Starting Gazelle Optimization Algorithm...\n",
      "GOA Iteration 1/30, Best Fitness: 0.0198\n",
      "GOA Iteration 2/30, Best Fitness: 0.0195\n",
      "GOA Iteration 3/30, Best Fitness: 0.0195\n",
      "GOA Iteration 4/30, Best Fitness: 0.0195\n",
      "GOA Iteration 5/30, Best Fitness: 0.0195\n",
      "GOA Iteration 6/30, Best Fitness: 0.0194\n",
      "GOA Iteration 7/30, Best Fitness: 0.0194\n",
      "GOA Iteration 8/30, Best Fitness: 0.0194\n",
      "GOA Iteration 9/30, Best Fitness: 0.0188\n",
      "GOA Iteration 10/30, Best Fitness: 0.0188\n",
      "GOA Iteration 11/30, Best Fitness: 0.0188\n",
      "GOA Iteration 12/30, Best Fitness: 0.0188\n",
      "GOA Iteration 13/30, Best Fitness: 0.0188\n",
      "GOA Iteration 14/30, Best Fitness: 0.0188\n",
      "GOA Iteration 15/30, Best Fitness: 0.0186\n",
      "GOA Iteration 16/30, Best Fitness: 0.0186\n",
      "GOA Iteration 17/30, Best Fitness: 0.0186\n",
      "GOA Iteration 18/30, Best Fitness: 0.0186\n",
      "GOA Iteration 19/30, Best Fitness: 0.0186\n",
      "GOA Iteration 20/30, Best Fitness: 0.0186\n",
      "GOA Iteration 21/30, Best Fitness: 0.0186\n",
      "GOA Iteration 22/30, Best Fitness: 0.0186\n",
      "GOA Iteration 23/30, Best Fitness: 0.0186\n",
      "GOA Iteration 24/30, Best Fitness: 0.0186\n",
      "GOA Iteration 25/30, Best Fitness: 0.0186\n",
      "GOA Iteration 26/30, Best Fitness: 0.0186\n",
      "GOA Iteration 27/30, Best Fitness: 0.0186\n",
      "GOA Iteration 28/30, Best Fitness: 0.0186\n",
      "GOA Iteration 29/30, Best Fitness: 0.0186\n",
      "GOA Iteration 30/30, Best Fitness: 0.0186\n",
      "GOA optimization completed. Best fitness: 0.0186\n",
      "Best RF parameters: {'n_estimators': 67, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 1, 'random_state': 42, 'n_jobs': -1}\n",
      "Best XGB parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.20437206496840132, 'subsample': 0.7028166066019045, 'colsample_bytree': 1.0, 'random_state': 42, 'n_jobs': -1, 'eval_metric': 'logloss'}\n",
      "Ensemble weights - RF: 0.300, XGB: 0.700\n",
      "Hybrid model training completed!\n",
      "Evaluating hybrid model...\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS - 8-CLASS REPRESENTATION\n",
      "================================================================================\n",
      "Hybrid Random Forest + XGBoost Model (GOA Optimized):\n",
      "  Accuracy:  0.9820\n",
      "  Precision: 0.9826\n",
      "  Recall:    0.9820\n",
      "  F1-Score:  0.9817\n",
      "\n",
      "Optimized Hyperparameters:\n",
      "  RF n_estimators: 67\n",
      "  RF max_depth: 6\n",
      "  XGB n_estimators: 200\n",
      "  XGB learning_rate: 0.204\n",
      "  Ensemble weights - RF: 0.300, XGB: 0.700\n",
      "Starting CICIoT2023 ML Algorithm with 34-class representation...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (253575, 47)\n",
      "Columns: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "Target column: label\n",
      "Target distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Handling missing values and duplicates...\n",
      "Removed 0 duplicate rows\n",
      "Final dataset shape after cleaning: (253575, 47)\n",
      "Handling outliers...\n",
      "Creating 34-class representation...\n",
      "Unique classes in dataset: ['DDoS-ICMP_Flood' 'DDoS-PSHACK_Flood' 'DoS-UDP_Flood' 'DDoS-SYN_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'DDoS-RSTFINFlood' 'Mirai-greip_flood'\n",
      " 'DDoS-SynonymousIP_Flood' 'Mirai-greeth_flood' 'DDoS-TCP_Flood'\n",
      " 'DDoS-UDP_Flood' 'DDoS-ICMP_Fragmentation' 'DoS-TCP_Flood' 'Recon-OSScan'\n",
      " 'DDoS-UDP_Fragmentation' 'MITM-ArpSpoofing' 'Mirai-udpplain'\n",
      " 'DoS-SYN_Flood' 'BenignTraffic' 'Recon-HostDiscovery' 'DDoS-SlowLoris'\n",
      " 'Recon-PingSweep' 'DNS_Spoofing' 'DoS-HTTP_Flood' 'DDoS-HTTP_Flood'\n",
      " 'VulnerabilityScan' 'DictionaryBruteForce' 'Recon-PortScan'\n",
      " 'SqlInjection' 'CommandInjection' 'XSS' 'BrowserHijacking'\n",
      " 'Uploading_Attack' 'Backdoor_Malware']\n",
      "Class counts: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "34-class: 34 classes, target samples per class: 7458\n",
      "Original class distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Final 34-class dataset shape: (126178, 47)\n",
      "Class distribution:\n",
      "label\n",
      "DDoS-ICMP_Flood            7458\n",
      "Mirai-greeth_flood         7458\n",
      "BenignTraffic              7458\n",
      "DDoS-PSHACK_Flood          7458\n",
      "Mirai-udpplain             7458\n",
      "DoS-TCP_Flood              7458\n",
      "DDoS-UDP_Flood             7458\n",
      "DDoS-TCP_Flood             7458\n",
      "DoS-SYN_Flood              7458\n",
      "DDoS-SynonymousIP_Flood    7458\n",
      "Mirai-greip_flood          7458\n",
      "DDoS-RSTFINFlood           7458\n",
      "DDoS-SYN_Flood             7458\n",
      "DoS-UDP_Flood              7458\n",
      "DDoS-ICMP_Fragmentation    4880\n",
      "MITM-ArpSpoofing           3426\n",
      "DDoS-ACK_Fragmentation     3068\n",
      "DDoS-UDP_Fragmentation     2986\n",
      "DNS_Spoofing               1944\n",
      "Recon-HostDiscovery        1434\n",
      "Recon-OSScan               1014\n",
      "Recon-PortScan              850\n",
      "DoS-HTTP_Flood              802\n",
      "VulnerabilityScan           400\n",
      "DDoS-HTTP_Flood             266\n",
      "DDoS-SlowLoris              254\n",
      "DictionaryBruteForce        142\n",
      "BrowserHijacking             72\n",
      "XSS                          60\n",
      "CommandInjection             52\n",
      "Backdoor_Malware             46\n",
      "SqlInjection                 34\n",
      "Recon-PingSweep              30\n",
      "Uploading_Attack              6\n",
      "Name: count, dtype: int64\n",
      "Training set size: 80753\n",
      "Validation set size: 20189\n",
      "Test set size: 25236\n",
      "Normalizing features...\n",
      "Training hybrid Random Forest + XGBoost model with GOA optimization...\n",
      "Starting Gazelle Optimization Algorithm...\n",
      "GOA Iteration 1/30, Best Fitness: 0.0318\n",
      "GOA Iteration 2/30, Best Fitness: 0.0307\n",
      "GOA Iteration 3/30, Best Fitness: 0.0305\n",
      "GOA Iteration 4/30, Best Fitness: 0.0299\n",
      "GOA Iteration 5/30, Best Fitness: 0.0299\n",
      "GOA Iteration 6/30, Best Fitness: 0.0299\n",
      "GOA Iteration 7/30, Best Fitness: 0.0299\n",
      "GOA Iteration 8/30, Best Fitness: 0.0299\n",
      "GOA Iteration 9/30, Best Fitness: 0.0299\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GazelleOptimizationAlgorithm:\n",
    "    \"\"\"\n",
    "    Implementation of Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, population_size=30, max_iterations=50, dim=10):\n",
    "        self.population_size = population_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.dim = dim\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        \n",
    "    def initialize_population(self, bounds):\n",
    "        \"\"\"Initialize gazelle population within bounds\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            gazelle = []\n",
    "            for i in range(self.dim):\n",
    "                lower, upper = bounds[i]\n",
    "                gazelle.append(np.random.uniform(lower, upper))\n",
    "            population.append(gazelle)\n",
    "        return np.array(population)\n",
    "    \n",
    "    def fitness_function(self, position, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Fitness function for hyperparameter optimization\n",
    "        Returns validation accuracy of hybrid model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode hyperparameters from position\n",
    "            rf_n_estimators = int(position[0])\n",
    "            rf_max_depth = int(position[1]) if position[1] > 0 else None\n",
    "            rf_min_samples_split = int(position[2])\n",
    "            rf_min_samples_leaf = int(position[3])\n",
    "            \n",
    "            xgb_n_estimators = int(position[4])\n",
    "            xgb_max_depth = int(position[5])\n",
    "            xgb_learning_rate = position[6]\n",
    "            xgb_subsample = position[7]\n",
    "            xgb_colsample_bytree = position[8]\n",
    "            \n",
    "            # Weight for ensemble\n",
    "            rf_weight = position[9]\n",
    "            xgb_weight = 1 - rf_weight\n",
    "            \n",
    "            # Train Random Forest\n",
    "            rf_model = RandomForestClassifier(\n",
    "                n_estimators=rf_n_estimators,\n",
    "                max_depth=rf_max_depth,\n",
    "                min_samples_split=rf_min_samples_split,\n",
    "                min_samples_leaf=rf_min_samples_leaf,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            rf_pred_proba = rf_model.predict_proba(X_val)\n",
    "            \n",
    "            # Train XGBoost\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                n_estimators=xgb_n_estimators,\n",
    "                max_depth=xgb_max_depth,\n",
    "                learning_rate=xgb_learning_rate,\n",
    "                subsample=xgb_subsample,\n",
    "                colsample_bytree=xgb_colsample_bytree,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_pred_proba = xgb_model.predict_proba(X_val)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            ensemble_pred_proba = rf_weight * rf_pred_proba + xgb_weight * xgb_pred_proba\n",
    "            ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_val, ensemble_pred)\n",
    "            \n",
    "            # Return negative accuracy for minimization\n",
    "            return 1 - accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fitness function: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def optimize(self, X_train, y_train, X_val, y_val, bounds):\n",
    "        \"\"\"\n",
    "        Main optimization loop for GOA\n",
    "        \"\"\"\n",
    "        print(\"Starting Gazelle Optimization Algorithm...\")\n",
    "        \n",
    "        # Initialize population\n",
    "        population = self.initialize_population(bounds)\n",
    "        fitness_values = []\n",
    "        \n",
    "        # Evaluate initial population\n",
    "        for i, gazelle in enumerate(population):\n",
    "            fitness = self.fitness_function(gazelle, X_train, y_train, X_val, y_val)\n",
    "            fitness_values.append(fitness)\n",
    "            \n",
    "            if fitness < self.best_fitness:\n",
    "                self.best_fitness = fitness\n",
    "                self.best_position = gazelle.copy()\n",
    "        \n",
    "        fitness_values = np.array(fitness_values)\n",
    "        \n",
    "        # Main optimization loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"GOA Iteration {iteration + 1}/{self.max_iterations}, Best Fitness: {self.best_fitness:.4f}\")\n",
    "            \n",
    "            # Update positions based on GOA algorithm\n",
    "            for i in range(self.population_size):\n",
    "                # Select random gazelles\n",
    "                r1, r2 = np.random.choice(self.population_size, 2, replace=False)\n",
    "                while r1 == i or r2 == i:\n",
    "                    r1, r2 = np.random.choice(self.population_size, 2, replace=False)\n",
    "                \n",
    "                # Calculate step size\n",
    "                step_size = 2 * np.random.random() - 1\n",
    "                \n",
    "                # Update position\n",
    "                for j in range(self.dim):\n",
    "                    # Gazelle movement equation\n",
    "                    if np.random.random() < 0.5:\n",
    "                        # Exploration\n",
    "                        population[i][j] = population[i][j] + step_size * (population[r1][j] - population[r2][j])\n",
    "                    else:\n",
    "                        # Exploitation\n",
    "                        population[i][j] = population[i][j] + step_size * (self.best_position[j] - population[i][j])\n",
    "                    \n",
    "                    # Boundary handling\n",
    "                    lower, upper = bounds[j]\n",
    "                    population[i][j] = np.clip(population[i][j], lower, upper)\n",
    "                \n",
    "                # Evaluate new position\n",
    "                new_fitness = self.fitness_function(population[i], X_train, y_train, X_val, y_val)\n",
    "                \n",
    "                # Update if better\n",
    "                if new_fitness < fitness_values[i]:\n",
    "                    fitness_values[i] = new_fitness\n",
    "                    \n",
    "                    if new_fitness < self.best_fitness:\n",
    "                        self.best_fitness = new_fitness\n",
    "                        self.best_position = population[i].copy()\n",
    "        \n",
    "        print(f\"GOA optimization completed. Best fitness: {self.best_fitness:.4f}\")\n",
    "        return self.best_position\n",
    "\n",
    "class CICIoT2023MLAlgorithm:\n",
    "    \"\"\"\n",
    "    Implementation of ML algorithms for CICIoT2023 dataset with GOA and hybrid RF-XGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.rf_model = None\n",
    "        self.xgb_model = None\n",
    "        self.best_params = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the CICIoT2023 dataset\n",
    "        \"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "            \n",
    "            # Display basic info about the dataset\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Check for target column (assume it's the last column or named 'label')\n",
    "            if 'label' in df.columns:\n",
    "                target_col = 'label'\n",
    "            else:\n",
    "                target_col = df.columns[-1]\n",
    "                print(f\"Assuming '{target_col}' is the target column\")\n",
    "            \n",
    "            # Move target column to the end if it's not already there\n",
    "            if target_col != df.columns[-1]:\n",
    "                cols = [col for col in df.columns if col != target_col] + [target_col]\n",
    "                df = df[cols]\n",
    "            \n",
    "            print(f\"Target column: {target_col}\")\n",
    "            print(f\"Target distribution: {df[target_col].value_counts()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        # Handle missing values and outliers\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.handle_outliers(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Handle missing value with improved imputation strategies\n",
    "        \"\"\"\n",
    "        print(\"Handling missing values and duplicates...\")\n",
    "        \n",
    "        # Remove exact duplicates first\n",
    "        initial_shape = df.shape\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"Removed {initial_shape[0] - df.shape[0]} duplicate rows\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                if column == target_col:\n",
    "                    # For target column, drop rows with missing values\n",
    "                    df = df.dropna(subset=[column])\n",
    "                elif df[column].dtype in ['float64', 'int64']:\n",
    "                    # For numerical columns, use median imputation\n",
    "                    median_val = df[column].median()\n",
    "                    df[column].fillna(median_val, inplace=True)\n",
    "                else:\n",
    "                    # For categorical columns, use mode imputation\n",
    "                    mode_val = df[column].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df[column].fillna(mode_val[0], inplace=True)\n",
    "                    else:\n",
    "                        df[column].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        print(f\"Final dataset shape after cleaning: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df):\n",
    "        \"\"\"\n",
    "        Handle outliers using IQR method\n",
    "        \"\"\"\n",
    "        print(\"Handling outliers...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        numerical_columns = [col for col in numerical_columns if col != target_col]\n",
    "        \n",
    "        for column in numerical_columns:\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Cap outliers instead of removing them\n",
    "                df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "                df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_class_representation(self, df, representation_type='2-class'):\n",
    "        \"\"\"\n",
    "        Create different class representations as specified\n",
    "        \"\"\"\n",
    "        print(f\"Creating {representation_type} representation...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        # First, let's examine the actual class labels in the dataset\n",
    "        unique_classes = df[target_col].unique()\n",
    "        print(f\"Unique classes in dataset: {unique_classes}\")\n",
    "        print(f\"Class counts: {df[target_col].value_counts()}\")\n",
    "        \n",
    "        if representation_type == '2-class':\n",
    "            # 2-Class: Normal vs Attack (all malicious classes as 'Attack')\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Identify normal/benign traffic - check for common variations\n",
    "            normal_labels = ['Normal', 'BENIGN', 'Benign', 'normal', 'benign']\n",
    "            normal_class = None\n",
    "            \n",
    "            for label in normal_labels:\n",
    "                if label in unique_classes:\n",
    "                    normal_class = label\n",
    "                    break\n",
    "            \n",
    "            if normal_class is None:\n",
    "                # If no clear normal class, treat the most frequent class as normal\n",
    "                # or use a heuristic based on class names\n",
    "                class_counts = df[target_col].value_counts()\n",
    "                potential_normal = class_counts.index[0]  # Most frequent class\n",
    "                \n",
    "                # Check if any class name suggests it's normal/benign\n",
    "                for class_name in class_counts.index:\n",
    "                    if any(keyword in str(class_name).lower() for keyword in ['normal', 'benign', 'legitimate']):\n",
    "                        potential_normal = class_name\n",
    "                        break\n",
    "                \n",
    "                normal_class = potential_normal\n",
    "                print(f\"No explicit 'Normal' class found. Using '{normal_class}' as normal class.\")\n",
    "            \n",
    "            # Create binary classification\n",
    "            df_copy[target_col] = df_copy[target_col].apply(\n",
    "                lambda x: 'Normal' if x == normal_class else 'Attack'\n",
    "            )\n",
    "            \n",
    "            # Random sampling to create balanced dataset with 8450 samples per class\n",
    "            normal_samples = df_copy[df_copy[target_col] == 'Normal']\n",
    "            attack_samples = df_copy[df_copy[target_col] == 'Attack']\n",
    "            \n",
    "            print(f\"Normal samples: {len(normal_samples)}, Attack samples: {len(attack_samples)}\")\n",
    "            \n",
    "            # Check if we have enough samples\n",
    "            if len(normal_samples) == 0:\n",
    "                raise ValueError(\"No normal/benign samples found in the dataset!\")\n",
    "            if len(attack_samples) == 0:\n",
    "                raise ValueError(\"No attack samples found in the dataset!\")\n",
    "            \n",
    "            # Sample 8450 from each class\n",
    "            target_samples = min(8450, len(normal_samples), len(attack_samples))\n",
    "            if target_samples < 8450:\n",
    "                print(f\"Warning: Insufficient samples. Using {target_samples} samples per class instead of 8450\")\n",
    "            \n",
    "            if len(normal_samples) >= target_samples:\n",
    "                normal_samples = normal_samples.sample(n=target_samples, random_state=42)\n",
    "            else:\n",
    "                normal_samples = resample(normal_samples, n_samples=target_samples, random_state=42, replace=True)\n",
    "            \n",
    "            if len(attack_samples) >= target_samples:\n",
    "                attack_samples = attack_samples.sample(n=target_samples, random_state=42)\n",
    "            else:\n",
    "                attack_samples = resample(attack_samples, n_samples=target_samples, random_state=42, replace=True)\n",
    "            \n",
    "            df_balanced = pd.concat([normal_samples, attack_samples], ignore_index=True)\n",
    "            \n",
    "        elif representation_type == '8-class':\n",
    "            # 8-Class: Group attacks into 8 categories\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Create attack mapping based on actual classes in the dataset\n",
    "            attack_mapping = {}\n",
    "            \n",
    "            # Find normal class\n",
    "            normal_labels = ['Normal', 'BENIGN', 'Benign', 'normal', 'benign']\n",
    "            normal_class = None\n",
    "            for label in normal_labels:\n",
    "                if label in unique_classes:\n",
    "                    normal_class = label\n",
    "                    break\n",
    "            \n",
    "            if normal_class:\n",
    "                attack_mapping['Normal'] = [normal_class]\n",
    "            \n",
    "            # Group other classes based on keywords\n",
    "            remaining_classes = [c for c in unique_classes if c != normal_class]\n",
    "            \n",
    "            # Initialize attack categories\n",
    "            attack_categories = {\n",
    "                'DDoS': [],\n",
    "                'DoS': [],\n",
    "                'Recon': [],\n",
    "                'Web': [],\n",
    "                'BruteForce': [],\n",
    "                'Spoofing': [],\n",
    "                'Mirai': []\n",
    "            }\n",
    "            \n",
    "            # Classify attacks based on class names\n",
    "            for class_name in remaining_classes:\n",
    "                class_name_lower = str(class_name).lower()\n",
    "                \n",
    "                if 'ddos' in class_name_lower:\n",
    "                    attack_categories['DDoS'].append(class_name)\n",
    "                elif 'dos' in class_name_lower:\n",
    "                    attack_categories['DoS'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['recon', 'scan', 'ping', 'port']):\n",
    "                    attack_categories['Recon'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['web', 'sql', 'xss', 'injection']):\n",
    "                    attack_categories['Web'].append(class_name)\n",
    "                elif 'brute' in class_name_lower or 'force' in class_name_lower:\n",
    "                    attack_categories['BruteForce'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['spoof', 'mitm', 'arp']):\n",
    "                    attack_categories['Spoofing'].append(class_name)\n",
    "                elif 'mirai' in class_name_lower:\n",
    "                    attack_categories['Mirai'].append(class_name)\n",
    "                else:\n",
    "                    # Put remaining attacks in the most appropriate category or create \"Other\"\n",
    "                    attack_categories['DDoS'].append(class_name)  # Default to DDoS\n",
    "            \n",
    "            # Apply mapping\n",
    "            for new_class, old_classes in attack_categories.items():\n",
    "                if old_classes:  # Only apply if there are classes to map\n",
    "                    df_copy[target_col] = df_copy[target_col].replace(old_classes, new_class)\n",
    "            \n",
    "            # Add normal class mapping\n",
    "            if normal_class:\n",
    "                df_copy[target_col] = df_copy[target_col].replace(normal_class, 'Normal')\n",
    "            \n",
    "            # Remove empty categories\n",
    "            remaining_classes = df_copy[target_col].unique()\n",
    "            target_samples = min(33800, len(df_copy) // len(remaining_classes))\n",
    "            \n",
    "            print(f\"8-class categories: {remaining_classes}\")\n",
    "            print(f\"Target samples per class: {target_samples}\")\n",
    "            \n",
    "            # Balance dataset using SMOTE and undersampling\n",
    "            df_balanced = self.balance_multiclass_dataset(df_copy, target_samples=target_samples, use_smote=True)\n",
    "            \n",
    "        elif representation_type == '34-class':\n",
    "            # 34-Class: Keep all original classes\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Calculate target samples based on dataset size and number of classes\n",
    "            total_classes = len(unique_classes)\n",
    "            target_samples = min(84500, len(df_copy) // total_classes)\n",
    "            \n",
    "            if target_samples < 100:\n",
    "                target_samples = min(1000, len(df_copy) // total_classes)\n",
    "            \n",
    "            print(f\"34-class: {total_classes} classes, target samples per class: {target_samples}\")\n",
    "            \n",
    "            # Balance dataset using SMOTE and undersampling\n",
    "            df_balanced = self.balance_multiclass_dataset(df_copy, target_samples=target_samples, use_smote=True)\n",
    "        \n",
    "        print(f\"Final {representation_type} dataset shape: {df_balanced.shape}\")\n",
    "        print(f\"Class distribution:\")\n",
    "        print(df_balanced[target_col].value_counts())\n",
    "        \n",
    "        return df_balanced\n",
    "    \n",
    "    def balance_multiclass_dataset(self, df, target_samples, use_smote=True):\n",
    "        \"\"\"\n",
    "        Balance multiclass dataset using undersampling and SMOTE\n",
    "        \"\"\"\n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        # Check current class distribution\n",
    "        class_counts = df[target_col].value_counts()\n",
    "        print(f\"Original class distribution: {class_counts}\")\n",
    "        \n",
    "        # Ensure target_samples is reasonable\n",
    "        min_class_size = class_counts.min()\n",
    "        max_class_size = class_counts.max()\n",
    "        \n",
    "        if target_samples > max_class_size:\n",
    "            target_samples = max_class_size\n",
    "            print(f\"Adjusted target samples to {target_samples} (max available)\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop(target_col, axis=1)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Apply undersampling first for majority classes\n",
    "        df_balanced_list = []\n",
    "        for class_label in y.unique():\n",
    "            class_data = df[df[target_col] == class_label]\n",
    "            \n",
    "            if len(class_data) > target_samples:\n",
    "                # Undersample majority class\n",
    "                class_data = class_data.sample(n=target_samples, random_state=42)\n",
    "            elif len(class_data) < target_samples and use_smote:\n",
    "                # Keep minority classes as-is for now, SMOTE will handle them\n",
    "                pass\n",
    "            \n",
    "            df_balanced_list.append(class_data)\n",
    "        \n",
    "        df_undersampled = pd.concat(df_balanced_list, ignore_index=True)\n",
    "        \n",
    "        if use_smote:\n",
    "            # Apply SMOTE for minority classes\n",
    "            X_under = df_undersampled.drop(target_col, axis=1)\n",
    "            y_under = df_undersampled[target_col]\n",
    "            \n",
    "            # Encode labels for SMOTE\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y_under)\n",
    "            \n",
    "            try:\n",
    "                # Create sampling strategy\n",
    "                unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
    "                sampling_strategy = {}\n",
    "                \n",
    "                for class_idx, count in zip(unique_classes, class_counts):\n",
    "                    if count < target_samples:\n",
    "                        # Only upsample if we have enough neighbors\n",
    "                        min_neighbors = min(5, count - 1) if count > 1 else 1\n",
    "                        if count >= min_neighbors:\n",
    "                            sampling_strategy[class_idx] = min(target_samples, count * 2)\n",
    "                \n",
    "                if sampling_strategy:\n",
    "                    # Adjust k_neighbors based on smallest class size\n",
    "                    min_samples = min([np.sum(y_encoded == cls) for cls in sampling_strategy.keys()])\n",
    "                    k_neighbors = min(3, max(1, min_samples - 1))\n",
    "                    \n",
    "                    smote = SMOTE(random_state=42, k_neighbors=k_neighbors, sampling_strategy=sampling_strategy)\n",
    "                    X_smote, y_smote = smote.fit_resample(X_under, y_encoded)\n",
    "                    \n",
    "                    # Decode labels back\n",
    "                    y_smote_decoded = le.inverse_transform(y_smote)\n",
    "                    \n",
    "                    # Combine back to dataframe\n",
    "                    df_balanced = pd.concat([\n",
    "                        pd.DataFrame(X_smote, columns=X_under.columns),\n",
    "                        pd.Series(y_smote_decoded, name=target_col)\n",
    "                    ], axis=1)\n",
    "                else:\n",
    "                    df_balanced = df_undersampled\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"SMOTE failed: {e}, using undersampled data\")\n",
    "                df_balanced = df_undersampled\n",
    "        else:\n",
    "            df_balanced = df_undersampled\n",
    "        \n",
    "        return df_balanced\n",
    "    \n",
    "    def normalize_features(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Normalize features using StandardScaler\n",
    "        \"\"\"\n",
    "        print(\"Normalizing features...\")\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled\n",
    "    \n",
    "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train hybrid RF-XGB model using GOA optimization\n",
    "        \"\"\"\n",
    "        print(\"Training hybrid Random Forest + XGBoost model with GOA optimization...\")\n",
    "        \n",
    "        # Encode labels\n",
    "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = self.label_encoder.transform(y_val)\n",
    "        \n",
    "        # Define hyperparameter bounds for GOA\n",
    "        bounds = [\n",
    "            (50, 200),    # RF n_estimators\n",
    "            (3, 20),      # RF max_depth\n",
    "            (2, 10),      # RF min_samples_split\n",
    "            (1, 5),       # RF min_samples_leaf\n",
    "            (50, 200),    # XGB n_estimators\n",
    "            (3, 10),      # XGB max_depth\n",
    "            (0.01, 0.3),  # XGB learning_rate\n",
    "            (0.6, 1.0),   # XGB subsample\n",
    "            (0.6, 1.0),   # XGB colsample_bytree\n",
    "            (0.3, 0.7)    # RF weight in ensemble\n",
    "        ]\n",
    "        \n",
    "        # Initialize and run GOA\n",
    "        goa = GazelleOptimizationAlgorithm(population_size=20, max_iterations=30, dim=10)\n",
    "        self.best_params = goa.optimize(X_train, y_train_encoded, X_val, y_val_encoded, bounds)\n",
    "        \n",
    "        # Train final models with best parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': int(self.best_params[0]),\n",
    "            'max_depth': int(self.best_params[1]) if self.best_params[1] > 0 else None,\n",
    "            'min_samples_split': int(self.best_params[2]),\n",
    "            'min_samples_leaf': int(self.best_params[3]),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        xgb_params = {\n",
    "            'n_estimators': int(self.best_params[4]),\n",
    "            'max_depth': int(self.best_params[5]),\n",
    "            'learning_rate': self.best_params[6],\n",
    "            'subsample': self.best_params[7],\n",
    "            'colsample_bytree': self.best_params[8],\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        self.rf_weight = self.best_params[9]\n",
    "        self.xgb_weight = 1 - self.rf_weight\n",
    "        \n",
    "        print(f\"Best RF parameters: {rf_params}\")\n",
    "        print(f\"Best XGB parameters: {xgb_params}\")\n",
    "        print(f\"Ensemble weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}\")\n",
    "        \n",
    "        # Train final models\n",
    "        self.rf_model = RandomForestClassifier(**rf_params)\n",
    "        self.xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        self.rf_model.fit(X_train, y_train_encoded)\n",
    "        self.xgb_model.fit(X_train, y_train_encoded)\n",
    "        \n",
    "        print(\"Hybrid model training completed!\")\n",
    "    \n",
    "    def predict_hybrid(self, X_test):\n",
    "        \"\"\"\n",
    "        Make predictions using hybrid model\n",
    "        \"\"\"\n",
    "        rf_pred_proba = self.rf_model.predict_proba(X_test)\n",
    "        xgb_pred_proba = self.xgb_model.predict_proba(X_test)\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_pred_proba = self.rf_weight * rf_pred_proba + self.xgb_weight * xgb_pred_proba\n",
    "        ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "        \n",
    "        # Decode labels back\n",
    "        ensemble_pred_decoded = self.label_encoder.inverse_transform(ensemble_pred)\n",
    "        \n",
    "        return ensemble_pred_decoded\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the hybrid model\n",
    "        \"\"\"\n",
    "        print(\"Evaluating hybrid model...\")\n",
    "        \n",
    "        y_pred = self.predict_hybrid(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        self.results = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def run_algorithm(self, file_path, representation_type='2-class'):\n",
    "        \"\"\"\n",
    "        Run the complete algorithm with GOA optimization\n",
    "        \"\"\"\n",
    "        print(f\"Starting CICIoT2023 ML Algorithm with {representation_type} representation...\")\n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        df = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Step 2: Create class representation\n",
    "        df_balanced = self.create_class_representation(df, representation_type)\n",
    "        \n",
    "        # Step 3: Prepare features and target\n",
    "        X = df_balanced.iloc[:, :-1].values\n",
    "        y = df_balanced.iloc[:, -1].values\n",
    "        \n",
    "        # Step 4: Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Split training data for validation (used in GOA)\n",
    "        X_train_opt, X_val, y_train_opt, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {X_train_opt.shape[0]}\")\n",
    "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Step 5: Normalize features\n",
    "        X_train_scaled, X_test_scaled = self.normalize_features(X_train_opt, X_test)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Step 6: Train hybrid model with GOA optimization\n",
    "        self.train_hybrid_model(X_train_scaled, y_train_opt, X_val_scaled, y_val)\n",
    "        \n",
    "        # Step 7: Evaluate model\n",
    "        results = self.evaluate_model(X_test_scaled, y_test)\n",
    "        \n",
    "        # Display results\n",
    "        self.display_results(representation_type)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, representation_type):\n",
    "        \"\"\"\n",
    "        Display evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"EVALUATION RESULTS - {representation_type.upper()} REPRESENTATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Hybrid Random Forest + XGBoost Model (GOA Optimized):\")\n",
    "        print(f\"  Accuracy:  {self.results['Accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {self.results['Precision']:.4f}\")\n",
    "        print(f\"  Recall:    {self.results['Recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {self.results['F1-Score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nOptimized Hyperparameters:\")\n",
    "        print(f\"  RF n_estimators: {int(self.best_params[0])}\")\n",
    "        print(f\"  RF max_depth: {int(self.best_params[1]) if self.best_params[1] > 0 else None}\")\n",
    "        print(f\"  XGB n_estimators: {int(self.best_params[4])}\")\n",
    "        print(f\"  XGB learning_rate: {self.best_params[6]:.3f}\")\n",
    "        print(f\"  Ensemble weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the algorithm\n",
    "    ml_algorithm = CICIoT2023MLAlgorithm()\n",
    "    \n",
    "    print(\"CICIoT2023 Dataset ML Algorithm with GOA and Hybrid RF-XGB\")\n",
    "    print(\"This implementation includes:\")\n",
    "    print(\"- Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\")\n",
    "    print(\"- Hybrid Random Forest + XGBoost ensemble model\")\n",
    "    print(\"- Three class representations: 2-class, 8-class, and 34-class\")\n",
    "    print(\"\\nTo use this algorithm:\")\n",
    "    print(\"1. Ensure you have the CICIoT2023 dataset in CSV format\")\n",
    "    print(\"2. Install required packages: pip install pandas scikit-learn imbalanced-learn xgboost\")\n",
    "    print(\"3. Run for different representations:\")\n",
    "    print(\"   - 2-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '2-class')\")\n",
    "    print(\"   - 8-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '8-class')\")\n",
    "    print(\"   - 34-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '34-class')\")\n",
    "    \n",
    "     # Uncomment the following lines and provide the correct path to run the algorithm\n",
    "    results_2class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '2-class')\n",
    "    results_8class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '8-class')\n",
    "    results_34class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '34-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f32c7-fd2d-4e66-a090-af0a06cc14e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CICIoT2023 Dataset ML Algorithm with GOA and Hybrid RF-XGB\n",
      "This implementation includes:\n",
      "- Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\n",
      "- Hybrid Random Forest + XGBoost ensemble model\n",
      "- Three class representations: 2-class, 8-class, and 34-class\n",
      "\n",
      "To use this algorithm:\n",
      "1. Ensure you have the CICIoT2023 dataset in CSV format\n",
      "2. Install required packages: pip install pandas scikit-learn imbalanced-learn xgboost\n",
      "3. Run for different representations:\n",
      "   - 2-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '2-class')\n",
      "   - 8-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '8-class')\n",
      "   - 34-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '34-class')\n",
      "Starting CICIoT2023 ML Algorithm with 34-class representation...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (253575, 47)\n",
      "Columns: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "Target column: label\n",
      "Target distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Handling missing values and duplicates...\n",
      "Removed 0 duplicate rows\n",
      "Final dataset shape after cleaning: (253575, 47)\n",
      "Handling outliers...\n",
      "Creating 34-class representation...\n",
      "Unique classes in dataset: ['DDoS-ICMP_Flood' 'DDoS-PSHACK_Flood' 'DoS-UDP_Flood' 'DDoS-SYN_Flood'\n",
      " 'DDoS-ACK_Fragmentation' 'DDoS-RSTFINFlood' 'Mirai-greip_flood'\n",
      " 'DDoS-SynonymousIP_Flood' 'Mirai-greeth_flood' 'DDoS-TCP_Flood'\n",
      " 'DDoS-UDP_Flood' 'DDoS-ICMP_Fragmentation' 'DoS-TCP_Flood' 'Recon-OSScan'\n",
      " 'DDoS-UDP_Fragmentation' 'MITM-ArpSpoofing' 'Mirai-udpplain'\n",
      " 'DoS-SYN_Flood' 'BenignTraffic' 'Recon-HostDiscovery' 'DDoS-SlowLoris'\n",
      " 'Recon-PingSweep' 'DNS_Spoofing' 'DoS-HTTP_Flood' 'DDoS-HTTP_Flood'\n",
      " 'VulnerabilityScan' 'DictionaryBruteForce' 'Recon-PortScan'\n",
      " 'SqlInjection' 'CommandInjection' 'XSS' 'BrowserHijacking'\n",
      " 'Uploading_Attack' 'Backdoor_Malware']\n",
      "Class counts: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "34-class: 34 classes, target samples per class: 7458\n",
      "Original class distribution: label\n",
      "DDoS-ICMP_Flood            39408\n",
      "DDoS-UDP_Flood             29523\n",
      "DDoS-TCP_Flood             24764\n",
      "DDoS-PSHACK_Flood          22151\n",
      "DDoS-SYN_Flood             22058\n",
      "DDoS-RSTFINFlood           22004\n",
      "DDoS-SynonymousIP_Flood    19265\n",
      "DoS-UDP_Flood              18023\n",
      "DoS-TCP_Flood              14319\n",
      "DoS-SYN_Flood              11074\n",
      "BenignTraffic               5882\n",
      "Mirai-greeth_flood          5301\n",
      "Mirai-udpplain              4797\n",
      "Mirai-greip_flood           4123\n",
      "DDoS-ICMP_Fragmentation     2440\n",
      "MITM-ArpSpoofing            1713\n",
      "DDoS-ACK_Fragmentation      1534\n",
      "DDoS-UDP_Fragmentation      1493\n",
      "DNS_Spoofing                 972\n",
      "Recon-HostDiscovery          717\n",
      "Recon-OSScan                 507\n",
      "Recon-PortScan               425\n",
      "DoS-HTTP_Flood               401\n",
      "VulnerabilityScan            200\n",
      "DDoS-HTTP_Flood              133\n",
      "DDoS-SlowLoris               127\n",
      "DictionaryBruteForce          71\n",
      "BrowserHijacking              36\n",
      "XSS                           30\n",
      "CommandInjection              26\n",
      "Backdoor_Malware              23\n",
      "SqlInjection                  17\n",
      "Recon-PingSweep               15\n",
      "Uploading_Attack               3\n",
      "Name: count, dtype: int64\n",
      "Final 34-class dataset shape: (126178, 47)\n",
      "Class distribution:\n",
      "label\n",
      "DDoS-ICMP_Flood            7458\n",
      "Mirai-greeth_flood         7458\n",
      "BenignTraffic              7458\n",
      "DDoS-PSHACK_Flood          7458\n",
      "Mirai-udpplain             7458\n",
      "DoS-TCP_Flood              7458\n",
      "DDoS-UDP_Flood             7458\n",
      "DDoS-TCP_Flood             7458\n",
      "DoS-SYN_Flood              7458\n",
      "DDoS-SynonymousIP_Flood    7458\n",
      "Mirai-greip_flood          7458\n",
      "DDoS-RSTFINFlood           7458\n",
      "DDoS-SYN_Flood             7458\n",
      "DoS-UDP_Flood              7458\n",
      "DDoS-ICMP_Fragmentation    4880\n",
      "MITM-ArpSpoofing           3426\n",
      "DDoS-ACK_Fragmentation     3068\n",
      "DDoS-UDP_Fragmentation     2986\n",
      "DNS_Spoofing               1944\n",
      "Recon-HostDiscovery        1434\n",
      "Recon-OSScan               1014\n",
      "Recon-PortScan              850\n",
      "DoS-HTTP_Flood              802\n",
      "VulnerabilityScan           400\n",
      "DDoS-HTTP_Flood             266\n",
      "DDoS-SlowLoris              254\n",
      "DictionaryBruteForce        142\n",
      "BrowserHijacking             72\n",
      "XSS                          60\n",
      "CommandInjection             52\n",
      "Backdoor_Malware             46\n",
      "SqlInjection                 34\n",
      "Recon-PingSweep              30\n",
      "Uploading_Attack              6\n",
      "Name: count, dtype: int64\n",
      "Training set size: 80753\n",
      "Validation set size: 20189\n",
      "Test set size: 25236\n",
      "Normalizing features...\n",
      "Training hybrid Random Forest + XGBoost model with GOA optimization...\n",
      "Starting Gazelle Optimization Algorithm...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GazelleOptimizationAlgorithm:\n",
    "    \"\"\"\n",
    "    Implementation of Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, population_size=30, max_iterations=50, dim=10):\n",
    "        self.population_size = population_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.dim = dim\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        \n",
    "    def initialize_population(self, bounds):\n",
    "        \"\"\"Initialize gazelle population within bounds\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            gazelle = []\n",
    "            for i in range(self.dim):\n",
    "                lower, upper = bounds[i]\n",
    "                gazelle.append(np.random.uniform(lower, upper))\n",
    "            population.append(gazelle)\n",
    "        return np.array(population)\n",
    "    \n",
    "    def fitness_function(self, position, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Fitness function for hyperparameter optimization\n",
    "        Returns validation accuracy of hybrid model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode hyperparameters from position\n",
    "            rf_n_estimators = int(position[0])\n",
    "            rf_max_depth = int(position[1]) if position[1] > 0 else None\n",
    "            rf_min_samples_split = int(position[2])\n",
    "            rf_min_samples_leaf = int(position[3])\n",
    "            \n",
    "            xgb_n_estimators = int(position[4])\n",
    "            xgb_max_depth = int(position[5])\n",
    "            xgb_learning_rate = position[6]\n",
    "            xgb_subsample = position[7]\n",
    "            xgb_colsample_bytree = position[8]\n",
    "            \n",
    "            # Weight for ensemble\n",
    "            rf_weight = position[9]\n",
    "            xgb_weight = 1 - rf_weight\n",
    "            \n",
    "            # Train Random Forest\n",
    "            rf_model = RandomForestClassifier(\n",
    "                n_estimators=rf_n_estimators,\n",
    "                max_depth=rf_max_depth,\n",
    "                min_samples_split=rf_min_samples_split,\n",
    "                min_samples_leaf=rf_min_samples_leaf,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf_model.fit(X_train, y_train)\n",
    "            rf_pred_proba = rf_model.predict_proba(X_val)\n",
    "            \n",
    "            # Train XGBoost\n",
    "            xgb_model = xgb.XGBClassifier(\n",
    "                n_estimators=xgb_n_estimators,\n",
    "                max_depth=xgb_max_depth,\n",
    "                learning_rate=xgb_learning_rate,\n",
    "                subsample=xgb_subsample,\n",
    "                colsample_bytree=xgb_colsample_bytree,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            xgb_pred_proba = xgb_model.predict_proba(X_val)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            ensemble_pred_proba = rf_weight * rf_pred_proba + xgb_weight * xgb_pred_proba\n",
    "            ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_val, ensemble_pred)\n",
    "            \n",
    "            # Return negative accuracy for minimization\n",
    "            return 1 - accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fitness function: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def optimize(self, X_train, y_train, X_val, y_val, bounds):\n",
    "        \"\"\"\n",
    "        Main optimization loop for GOA\n",
    "        \"\"\"\n",
    "        print(\"Starting Gazelle Optimization Algorithm...\")\n",
    "        \n",
    "        # Initialize population\n",
    "        population = self.initialize_population(bounds)\n",
    "        fitness_values = []\n",
    "        \n",
    "        # Evaluate initial population\n",
    "        for i, gazelle in enumerate(population):\n",
    "            fitness = self.fitness_function(gazelle, X_train, y_train, X_val, y_val)\n",
    "            fitness_values.append(fitness)\n",
    "            \n",
    "            if fitness < self.best_fitness:\n",
    "                self.best_fitness = fitness\n",
    "                self.best_position = gazelle.copy()\n",
    "        \n",
    "        fitness_values = np.array(fitness_values)\n",
    "        \n",
    "        # Main optimization loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"GOA Iteration {iteration + 1}/{self.max_iterations}, Best Fitness: {self.best_fitness:.4f}\")\n",
    "            \n",
    "            # Update positions based on GOA algorithm\n",
    "            for i in range(self.population_size):\n",
    "                # Select random gazelles\n",
    "                r1, r2 = np.random.choice(self.population_size, 2, replace=False)\n",
    "                while r1 == i or r2 == i:\n",
    "                    r1, r2 = np.random.choice(self.population_size, 2, replace=False)\n",
    "                \n",
    "                # Calculate step size\n",
    "                step_size = 2 * np.random.random() - 1\n",
    "                \n",
    "                # Update position\n",
    "                for j in range(self.dim):\n",
    "                    # Gazelle movement equation\n",
    "                    if np.random.random() < 0.5:\n",
    "                        # Exploration\n",
    "                        population[i][j] = population[i][j] + step_size * (population[r1][j] - population[r2][j])\n",
    "                    else:\n",
    "                        # Exploitation\n",
    "                        population[i][j] = population[i][j] + step_size * (self.best_position[j] - population[i][j])\n",
    "                    \n",
    "                    # Boundary handling\n",
    "                    lower, upper = bounds[j]\n",
    "                    population[i][j] = np.clip(population[i][j], lower, upper)\n",
    "                \n",
    "                # Evaluate new position\n",
    "                new_fitness = self.fitness_function(population[i], X_train, y_train, X_val, y_val)\n",
    "                \n",
    "                # Update if better\n",
    "                if new_fitness < fitness_values[i]:\n",
    "                    fitness_values[i] = new_fitness\n",
    "                    \n",
    "                    if new_fitness < self.best_fitness:\n",
    "                        self.best_fitness = new_fitness\n",
    "                        self.best_position = population[i].copy()\n",
    "        \n",
    "        print(f\"GOA optimization completed. Best fitness: {self.best_fitness:.4f}\")\n",
    "        return self.best_position\n",
    "\n",
    "class CICIoT2023MLAlgorithm:\n",
    "    \"\"\"\n",
    "    Implementation of ML algorithms for CICIoT2023 dataset with GOA and hybrid RF-XGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.rf_model = None\n",
    "        self.xgb_model = None\n",
    "        self.best_params = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess the CICIoT2023 dataset\n",
    "        \"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "            \n",
    "            # Display basic info about the dataset\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Check for target column (assume it's the last column or named 'label')\n",
    "            if 'label' in df.columns:\n",
    "                target_col = 'label'\n",
    "            else:\n",
    "                target_col = df.columns[-1]\n",
    "                print(f\"Assuming '{target_col}' is the target column\")\n",
    "            \n",
    "            # Move target column to the end if it's not already there\n",
    "            if target_col != df.columns[-1]:\n",
    "                cols = [col for col in df.columns if col != target_col] + [target_col]\n",
    "                df = df[cols]\n",
    "            \n",
    "            print(f\"Target column: {target_col}\")\n",
    "            print(f\"Target distribution: {df[target_col].value_counts()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        # Handle missing values and outliers\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.handle_outliers(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Handle missing value with improved imputation strategies\n",
    "        \"\"\"\n",
    "        print(\"Handling missing values and duplicates...\")\n",
    "        \n",
    "        # Remove exact duplicates first\n",
    "        initial_shape = df.shape\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"Removed {initial_shape[0] - df.shape[0]} duplicate rows\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                if column == target_col:\n",
    "                    # For target column, drop rows with missing values\n",
    "                    df = df.dropna(subset=[column])\n",
    "                elif df[column].dtype in ['float64', 'int64']:\n",
    "                    # For numerical columns, use median imputation\n",
    "                    median_val = df[column].median()\n",
    "                    df[column].fillna(median_val, inplace=True)\n",
    "                else:\n",
    "                    # For categorical columns, use mode imputation\n",
    "                    mode_val = df[column].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df[column].fillna(mode_val[0], inplace=True)\n",
    "                    else:\n",
    "                        df[column].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        print(f\"Final dataset shape after cleaning: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df):\n",
    "        \"\"\"\n",
    "        Handle outliers using IQR method\n",
    "        \"\"\"\n",
    "        print(\"Handling outliers...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        numerical_columns = [col for col in numerical_columns if col != target_col]\n",
    "        \n",
    "        for column in numerical_columns:\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Cap outliers instead of removing them\n",
    "                df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "                df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_class_representation(self, df, representation_type='2-class'):\n",
    "        \"\"\"\n",
    "        Create different class representations as specified\n",
    "        \"\"\"\n",
    "        print(f\"Creating {representation_type} representation...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        # First, let's examine the actual class labels in the dataset\n",
    "        unique_classes = df[target_col].unique()\n",
    "        print(f\"Unique classes in dataset: {unique_classes}\")\n",
    "        print(f\"Class counts: {df[target_col].value_counts()}\")\n",
    "        \n",
    "        if representation_type == '2-class':\n",
    "            # 2-Class: Normal vs Attack (all malicious classes as 'Attack')\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Identify normal/benign traffic - check for common variations\n",
    "            normal_labels = ['Normal', 'BENIGN', 'Benign', 'normal', 'benign']\n",
    "            normal_class = None\n",
    "            \n",
    "            for label in normal_labels:\n",
    "                if label in unique_classes:\n",
    "                    normal_class = label\n",
    "                    break\n",
    "            \n",
    "            if normal_class is None:\n",
    "                # If no clear normal class, treat the most frequent class as normal\n",
    "                # or use a heuristic based on class names\n",
    "                class_counts = df[target_col].value_counts()\n",
    "                potential_normal = class_counts.index[0]  # Most frequent class\n",
    "                \n",
    "                # Check if any class name suggests it's normal/benign\n",
    "                for class_name in class_counts.index:\n",
    "                    if any(keyword in str(class_name).lower() for keyword in ['normal', 'benign', 'legitimate']):\n",
    "                        potential_normal = class_name\n",
    "                        break\n",
    "                \n",
    "                normal_class = potential_normal\n",
    "                print(f\"No explicit 'Normal' class found. Using '{normal_class}' as normal class.\")\n",
    "            \n",
    "            # Create binary classification\n",
    "            df_copy[target_col] = df_copy[target_col].apply(\n",
    "                lambda x: 'Normal' if x == normal_class else 'Attack'\n",
    "            )\n",
    "            \n",
    "            # Random sampling to create balanced dataset with 8450 samples per class\n",
    "            normal_samples = df_copy[df_copy[target_col] == 'Normal']\n",
    "            attack_samples = df_copy[df_copy[target_col] == 'Attack']\n",
    "            \n",
    "            print(f\"Normal samples: {len(normal_samples)}, Attack samples: {len(attack_samples)}\")\n",
    "            \n",
    "            # Check if we have enough samples\n",
    "            if len(normal_samples) == 0:\n",
    "                raise ValueError(\"No normal/benign samples found in the dataset!\")\n",
    "            if len(attack_samples) == 0:\n",
    "                raise ValueError(\"No attack samples found in the dataset!\")\n",
    "            \n",
    "            # Sample 8450 from each class\n",
    "            target_samples = min(8450, len(normal_samples), len(attack_samples))\n",
    "            if target_samples < 8450:\n",
    "                print(f\"Warning: Insufficient samples. Using {target_samples} samples per class instead of 8450\")\n",
    "            \n",
    "            if len(normal_samples) >= target_samples:\n",
    "                normal_samples = normal_samples.sample(n=target_samples, random_state=42)\n",
    "            else:\n",
    "                normal_samples = resample(normal_samples, n_samples=target_samples, random_state=42, replace=True)\n",
    "            \n",
    "            if len(attack_samples) >= target_samples:\n",
    "                attack_samples = attack_samples.sample(n=target_samples, random_state=42)\n",
    "            else:\n",
    "                attack_samples = resample(attack_samples, n_samples=target_samples, random_state=42, replace=True)\n",
    "            \n",
    "            df_balanced = pd.concat([normal_samples, attack_samples], ignore_index=True)\n",
    "            \n",
    "        elif representation_type == '8-class':\n",
    "            # 8-Class: Group attacks into 8 categories\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Create attack mapping based on actual classes in the dataset\n",
    "            attack_mapping = {}\n",
    "            \n",
    "            # Find normal class\n",
    "            normal_labels = ['Normal', 'BENIGN', 'Benign', 'normal', 'benign']\n",
    "            normal_class = None\n",
    "            for label in normal_labels:\n",
    "                if label in unique_classes:\n",
    "                    normal_class = label\n",
    "                    break\n",
    "            \n",
    "            if normal_class:\n",
    "                attack_mapping['Normal'] = [normal_class]\n",
    "            \n",
    "            # Group other classes based on keywords\n",
    "            remaining_classes = [c for c in unique_classes if c != normal_class]\n",
    "            \n",
    "            # Initialize attack categories\n",
    "            attack_categories = {\n",
    "                'DDoS': [],\n",
    "                'DoS': [],\n",
    "                'Recon': [],\n",
    "                'Web': [],\n",
    "                'BruteForce': [],\n",
    "                'Spoofing': [],\n",
    "                'Mirai': []\n",
    "            }\n",
    "            \n",
    "            # Classify attacks based on class names\n",
    "            for class_name in remaining_classes:\n",
    "                class_name_lower = str(class_name).lower()\n",
    "                \n",
    "                if 'ddos' in class_name_lower:\n",
    "                    attack_categories['DDoS'].append(class_name)\n",
    "                elif 'dos' in class_name_lower:\n",
    "                    attack_categories['DoS'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['recon', 'scan', 'ping', 'port']):\n",
    "                    attack_categories['Recon'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['web', 'sql', 'xss', 'injection']):\n",
    "                    attack_categories['Web'].append(class_name)\n",
    "                elif 'brute' in class_name_lower or 'force' in class_name_lower:\n",
    "                    attack_categories['BruteForce'].append(class_name)\n",
    "                elif any(keyword in class_name_lower for keyword in ['spoof', 'mitm', 'arp']):\n",
    "                    attack_categories['Spoofing'].append(class_name)\n",
    "                elif 'mirai' in class_name_lower:\n",
    "                    attack_categories['Mirai'].append(class_name)\n",
    "                else:\n",
    "                    # Put remaining attacks in the most appropriate category or create \"Other\"\n",
    "                    attack_categories['DDoS'].append(class_name)  # Default to DDoS\n",
    "            \n",
    "            # Apply mapping\n",
    "            for new_class, old_classes in attack_categories.items():\n",
    "                if old_classes:  # Only apply if there are classes to map\n",
    "                    df_copy[target_col] = df_copy[target_col].replace(old_classes, new_class)\n",
    "            \n",
    "            # Add normal class mapping\n",
    "            if normal_class:\n",
    "                df_copy[target_col] = df_copy[target_col].replace(normal_class, 'Normal')\n",
    "            \n",
    "            # Remove empty categories\n",
    "            remaining_classes = df_copy[target_col].unique()\n",
    "            target_samples = min(33800, len(df_copy) // len(remaining_classes))\n",
    "            \n",
    "            print(f\"8-class categories: {remaining_classes}\")\n",
    "            print(f\"Target samples per class: {target_samples}\")\n",
    "            \n",
    "            # Balance dataset using SMOTE and undersampling\n",
    "            df_balanced = self.balance_multiclass_dataset(df_copy, target_samples=target_samples, use_smote=True)\n",
    "            \n",
    "        elif representation_type == '34-class':\n",
    "            # 34-Class: Keep all original classes\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Calculate target samples based on dataset size and number of classes\n",
    "            total_classes = len(unique_classes)\n",
    "            target_samples = min(84500, len(df_copy) // total_classes)\n",
    "            \n",
    "            if target_samples < 100:\n",
    "                target_samples = min(1000, len(df_copy) // total_classes)\n",
    "            \n",
    "            print(f\"34-class: {total_classes} classes, target samples per class: {target_samples}\")\n",
    "            \n",
    "            # Balance dataset using SMOTE and undersampling\n",
    "            df_balanced = self.balance_multiclass_dataset(df_copy, target_samples=target_samples, use_smote=True)\n",
    "        \n",
    "        print(f\"Final {representation_type} dataset shape: {df_balanced.shape}\")\n",
    "        print(f\"Class distribution:\")\n",
    "        print(df_balanced[target_col].value_counts())\n",
    "        \n",
    "        return df_balanced\n",
    "    \n",
    "    def balance_multiclass_dataset(self, df, target_samples, use_smote=True):\n",
    "        \"\"\"\n",
    "        Balance multiclass dataset using undersampling and SMOTE\n",
    "        \"\"\"\n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        # Check current class distribution\n",
    "        class_counts = df[target_col].value_counts()\n",
    "        print(f\"Original class distribution: {class_counts}\")\n",
    "        \n",
    "        # Ensure target_samples is reasonable\n",
    "        min_class_size = class_counts.min()\n",
    "        max_class_size = class_counts.max()\n",
    "        \n",
    "        if target_samples > max_class_size:\n",
    "            target_samples = max_class_size\n",
    "            print(f\"Adjusted target samples to {target_samples} (max available)\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = df.drop(target_col, axis=1)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Apply undersampling first for majority classes\n",
    "        df_balanced_list = []\n",
    "        for class_label in y.unique():\n",
    "            class_data = df[df[target_col] == class_label]\n",
    "            \n",
    "            if len(class_data) > target_samples:\n",
    "                # Undersample majority class\n",
    "                class_data = class_data.sample(n=target_samples, random_state=42)\n",
    "            elif len(class_data) < target_samples and use_smote:\n",
    "                # Keep minority classes as-is for now, SMOTE will handle them\n",
    "                pass\n",
    "            \n",
    "            df_balanced_list.append(class_data)\n",
    "        \n",
    "        df_undersampled = pd.concat(df_balanced_list, ignore_index=True)\n",
    "        \n",
    "        if use_smote:\n",
    "            # Apply SMOTE for minority classes\n",
    "            X_under = df_undersampled.drop(target_col, axis=1)\n",
    "            y_under = df_undersampled[target_col]\n",
    "            \n",
    "            # Encode labels for SMOTE\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y_under)\n",
    "            \n",
    "            try:\n",
    "                # Create sampling strategy\n",
    "                unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
    "                sampling_strategy = {}\n",
    "                \n",
    "                for class_idx, count in zip(unique_classes, class_counts):\n",
    "                    if count < target_samples:\n",
    "                        # Only upsample if we have enough neighbors\n",
    "                        min_neighbors = min(5, count - 1) if count > 1 else 1\n",
    "                        if count >= min_neighbors:\n",
    "                            sampling_strategy[class_idx] = min(target_samples, count * 2)\n",
    "                \n",
    "                if sampling_strategy:\n",
    "                    # Adjust k_neighbors based on smallest class size\n",
    "                    min_samples = min([np.sum(y_encoded == cls) for cls in sampling_strategy.keys()])\n",
    "                    k_neighbors = min(3, max(1, min_samples - 1))\n",
    "                    \n",
    "                    smote = SMOTE(random_state=42, k_neighbors=k_neighbors, sampling_strategy=sampling_strategy)\n",
    "                    X_smote, y_smote = smote.fit_resample(X_under, y_encoded)\n",
    "                    \n",
    "                    # Decode labels back\n",
    "                    y_smote_decoded = le.inverse_transform(y_smote)\n",
    "                    \n",
    "                    # Combine back to dataframe\n",
    "                    df_balanced = pd.concat([\n",
    "                        pd.DataFrame(X_smote, columns=X_under.columns),\n",
    "                        pd.Series(y_smote_decoded, name=target_col)\n",
    "                    ], axis=1)\n",
    "                else:\n",
    "                    df_balanced = df_undersampled\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"SMOTE failed: {e}, using undersampled data\")\n",
    "                df_balanced = df_undersampled\n",
    "        else:\n",
    "            df_balanced = df_undersampled\n",
    "        \n",
    "        return df_balanced\n",
    "    \n",
    "    def normalize_features(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Normalize features using StandardScaler\n",
    "        \"\"\"\n",
    "        print(\"Normalizing features...\")\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled\n",
    "    \n",
    "    def train_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Train hybrid RF-XGB model using GOA optimization\n",
    "        \"\"\"\n",
    "        print(\"Training hybrid Random Forest + XGBoost model with GOA optimization...\")\n",
    "        \n",
    "        # Encode labels\n",
    "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = self.label_encoder.transform(y_val)\n",
    "        \n",
    "        # Define hyperparameter bounds for GOA\n",
    "        bounds = [\n",
    "            (50, 200),    # RF n_estimators\n",
    "            (3, 20),      # RF max_depth\n",
    "            (2, 10),      # RF min_samples_split\n",
    "            (1, 5),       # RF min_samples_leaf\n",
    "            (50, 200),    # XGB n_estimators\n",
    "            (3, 10),      # XGB max_depth\n",
    "            (0.01, 0.3),  # XGB learning_rate\n",
    "            (0.6, 1.0),   # XGB subsample\n",
    "            (0.6, 1.0),   # XGB colsample_bytree\n",
    "            (0.3, 0.7)    # RF weight in ensemble\n",
    "        ]\n",
    "        \n",
    "        # Initialize and run GOA\n",
    "        goa = GazelleOptimizationAlgorithm(population_size=20, max_iterations=30, dim=10)\n",
    "        self.best_params = goa.optimize(X_train, y_train_encoded, X_val, y_val_encoded, bounds)\n",
    "        \n",
    "        # Train final models with best parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': int(self.best_params[0]),\n",
    "            'max_depth': int(self.best_params[1]) if self.best_params[1] > 0 else None,\n",
    "            'min_samples_split': int(self.best_params[2]),\n",
    "            'min_samples_leaf': int(self.best_params[3]),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        xgb_params = {\n",
    "            'n_estimators': int(self.best_params[4]),\n",
    "            'max_depth': int(self.best_params[5]),\n",
    "            'learning_rate': self.best_params[6],\n",
    "            'subsample': self.best_params[7],\n",
    "            'colsample_bytree': self.best_params[8],\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        self.rf_weight = self.best_params[9]\n",
    "        self.xgb_weight = 1 - self.rf_weight\n",
    "        \n",
    "        print(f\"Best RF parameters: {rf_params}\")\n",
    "        print(f\"Best XGB parameters: {xgb_params}\")\n",
    "        print(f\"Ensemble weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}\")\n",
    "        \n",
    "        # Train final models\n",
    "        self.rf_model = RandomForestClassifier(**rf_params)\n",
    "        self.xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        \n",
    "        self.rf_model.fit(X_train, y_train_encoded)\n",
    "        self.xgb_model.fit(X_train, y_train_encoded)\n",
    "        \n",
    "        print(\"Hybrid model training completed!\")\n",
    "    \n",
    "    def predict_hybrid(self, X_test):\n",
    "        \"\"\"\n",
    "        Make predictions using hybrid model\n",
    "        \"\"\"\n",
    "        rf_pred_proba = self.rf_model.predict_proba(X_test)\n",
    "        xgb_pred_proba = self.xgb_model.predict_proba(X_test)\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        ensemble_pred_proba = self.rf_weight * rf_pred_proba + self.xgb_weight * xgb_pred_proba\n",
    "        ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "        \n",
    "        # Decode labels back\n",
    "        ensemble_pred_decoded = self.label_encoder.inverse_transform(ensemble_pred)\n",
    "        \n",
    "        return ensemble_pred_decoded\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the hybrid model\n",
    "        \"\"\"\n",
    "        print(\"Evaluating hybrid model...\")\n",
    "        \n",
    "        y_pred = self.predict_hybrid(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        self.results = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def run_algorithm(self, file_path, representation_type='2-class'):\n",
    "        \"\"\"\n",
    "        Run the complete algorithm with GOA optimization\n",
    "        \"\"\"\n",
    "        print(f\"Starting CICIoT2023 ML Algorithm with {representation_type} representation...\")\n",
    "        \n",
    "        # Step 1: Load and preprocess data\n",
    "        df = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Step 2: Create class representation\n",
    "        df_balanced = self.create_class_representation(df, representation_type)\n",
    "        \n",
    "        # Step 3: Prepare features and target\n",
    "        X = df_balanced.iloc[:, :-1].values\n",
    "        y = df_balanced.iloc[:, -1].values\n",
    "        \n",
    "        # Step 4: Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Split training data for validation (used in GOA)\n",
    "        X_train_opt, X_val, y_train_opt, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {X_train_opt.shape[0]}\")\n",
    "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Step 5: Normalize features\n",
    "        X_train_scaled, X_test_scaled = self.normalize_features(X_train_opt, X_test)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Step 6: Train hybrid model with GOA optimization\n",
    "        self.train_hybrid_model(X_train_scaled, y_train_opt, X_val_scaled, y_val)\n",
    "        \n",
    "        # Step 7: Evaluate model\n",
    "        results = self.evaluate_model(X_test_scaled, y_test)\n",
    "        \n",
    "        # Display results\n",
    "        self.display_results(representation_type)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, representation_type):\n",
    "        \"\"\"\n",
    "        Display evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"EVALUATION RESULTS - {representation_type.upper()} REPRESENTATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Hybrid Random Forest + XGBoost Model (GOA Optimized):\")\n",
    "        print(f\"  Accuracy:  {self.results['Accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {self.results['Precision']:.4f}\")\n",
    "        print(f\"  Recall:    {self.results['Recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {self.results['F1-Score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nOptimized Hyperparameters:\")\n",
    "        print(f\"  RF n_estimators: {int(self.best_params[0])}\")\n",
    "        print(f\"  RF max_depth: {int(self.best_params[1]) if self.best_params[1] > 0 else None}\")\n",
    "        print(f\"  XGB n_estimators: {int(self.best_params[4])}\")\n",
    "        print(f\"  XGB learning_rate: {self.best_params[6]:.3f}\")\n",
    "        print(f\"  Ensemble weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the algorithm\n",
    "    ml_algorithm = CICIoT2023MLAlgorithm()\n",
    "    \n",
    "    print(\"CICIoT2023 Dataset ML Algorithm with GOA and Hybrid RF-XGB\")\n",
    "    print(\"This implementation includes:\")\n",
    "    print(\"- Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\")\n",
    "    print(\"- Hybrid Random Forest + XGBoost ensemble model\")\n",
    "    print(\"- Three class representations: 2-class, 8-class, and 34-class\")\n",
    "    print(\"\\nTo use this algorithm:\")\n",
    "    print(\"1. Ensure you have the CICIoT2023 dataset in CSV format\")\n",
    "    print(\"2. Install required packages: pip install pandas scikit-learn imbalanced-learn xgboost\")\n",
    "    print(\"3. Run for different representations:\")\n",
    "    print(\"   - 2-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '2-class')\")\n",
    "    print(\"   - 8-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '8-class')\")\n",
    "    print(\"   - 34-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '34-class')\")\n",
    "    \n",
    "     # Uncomment the following lines and provide the correct path to run the algorithm\n",
    "   # results_2class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '2-class')\n",
    "    #results_8class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '8-class')\n",
    "    results_34class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '34-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e379c8-4f43-4a63-ba8c-36fce2f48f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedGazelleOptimizationAlgorithm:\n",
    "    \"\"\"\n",
    "    Enhanced Gazelle Optimization Algorithm with adaptive mechanisms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, population_size=40, max_iterations=100, dim=12):\n",
    "        self.population_size = population_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.dim = dim\n",
    "        self.best_position = None\n",
    "        self.best_fitness = float('inf')\n",
    "        self.fitness_history = []\n",
    "        self.diversity_threshold = 0.1\n",
    "        \n",
    "    def initialize_population(self, bounds):\n",
    "        \"\"\"Initialize gazelle population with better diversity\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            gazelle = []\n",
    "            for i in range(self.dim):\n",
    "                lower, upper = bounds[i]\n",
    "                # Use different initialization strategies\n",
    "                if np.random.random() < 0.5:\n",
    "                    gazelle.append(np.random.uniform(lower, upper))\n",
    "                else:\n",
    "                    # Bias towards middle values for some parameters\n",
    "                    gazelle.append(np.random.normal((lower + upper) / 2, (upper - lower) / 6))\n",
    "                    gazelle[-1] = np.clip(gazelle[-1], lower, upper)\n",
    "            population.append(gazelle)\n",
    "        return np.array(population)\n",
    "    \n",
    "    def fitness_function(self, position, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Enhanced fitness function with cross-validation\"\"\"\n",
    "        try:\n",
    "            # Decode hyperparameters\n",
    "            rf_n_estimators = int(position[0])\n",
    "            rf_max_depth = int(position[1]) if position[1] > 0 else None\n",
    "            rf_min_samples_split = int(position[2])\n",
    "            rf_min_samples_leaf = int(position[3])\n",
    "            \n",
    "            xgb_n_estimators = int(position[4])\n",
    "            xgb_max_depth = int(position[5])\n",
    "            xgb_learning_rate = position[6]\n",
    "            xgb_subsample = position[7]\n",
    "            xgb_colsample_bytree = position[8]\n",
    "            \n",
    "            # New parameters for LightGBM\n",
    "            lgb_n_estimators = int(position[9])\n",
    "            lgb_learning_rate = position[10]\n",
    "            \n",
    "            # Ensemble weights\n",
    "            rf_weight = position[11] / 3\n",
    "            xgb_weight = position[11] / 3\n",
    "            lgb_weight = 1 - rf_weight - xgb_weight\n",
    "            \n",
    "            # Use 3-fold cross-validation for more robust evaluation\n",
    "            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                # Train Random Forest\n",
    "                rf_model = RandomForestClassifier(\n",
    "                    n_estimators=rf_n_estimators,\n",
    "                    max_depth=rf_max_depth,\n",
    "                    min_samples_split=rf_min_samples_split,\n",
    "                    min_samples_leaf=rf_min_samples_leaf,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    class_weight='balanced'\n",
    "                )\n",
    "                rf_model.fit(X_fold_train, y_fold_train)\n",
    "                rf_pred_proba = rf_model.predict_proba(X_fold_val)\n",
    "                \n",
    "                # Train XGBoost\n",
    "                xgb_model = xgb.XGBClassifier(\n",
    "                    n_estimators=xgb_n_estimators,\n",
    "                    max_depth=xgb_max_depth,\n",
    "                    learning_rate=xgb_learning_rate,\n",
    "                    subsample=xgb_subsample,\n",
    "                    colsample_bytree=xgb_colsample_bytree,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    eval_metric='logloss',\n",
    "                    scale_pos_weight=len(y_fold_train) / (2 * np.sum(y_fold_train == y_fold_train[0])) if len(np.unique(y_fold_train)) == 2 else 1\n",
    "                )\n",
    "                xgb_model.fit(X_fold_train, y_fold_train)\n",
    "                xgb_pred_proba = xgb_model.predict_proba(X_fold_val)\n",
    "                \n",
    "                # Train LightGBM\n",
    "                lgb_model = lgb.LGBMClassifier(\n",
    "                    n_estimators=lgb_n_estimators,\n",
    "                    learning_rate=lgb_learning_rate,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    class_weight='balanced',\n",
    "                    verbosity=-1\n",
    "                )\n",
    "                lgb_model.fit(X_fold_train, y_fold_train)\n",
    "                lgb_pred_proba = lgb_model.predict_proba(X_fold_val)\n",
    "                \n",
    "                # Ensemble prediction\n",
    "                ensemble_pred_proba = rf_weight * rf_pred_proba + xgb_weight * xgb_pred_proba + lgb_weight * lgb_pred_proba\n",
    "                ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "                \n",
    "                # Calculate F1-score (more robust than accuracy for imbalanced data)\n",
    "                f1 = f1_score(y_fold_val, ensemble_pred, average='weighted')\n",
    "                cv_scores.append(f1)\n",
    "            \n",
    "            # Return negative mean F1-score for minimization\n",
    "            return 1 - np.mean(cv_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fitness function: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def optimize(self, X_train, y_train, X_val, y_val, bounds):\n",
    "        \"\"\"Enhanced optimization with adaptive mechanisms\"\"\"\n",
    "        print(\"Starting Advanced Gazelle Optimization Algorithm...\")\n",
    "        \n",
    "        population = self.initialize_population(bounds)\n",
    "        fitness_values = []\n",
    "        \n",
    "        # Evaluate initial population\n",
    "        for i, gazelle in enumerate(population):\n",
    "            fitness = self.fitness_function(gazelle, X_train, y_train, X_val, y_val)\n",
    "            fitness_values.append(fitness)\n",
    "            \n",
    "            if fitness < self.best_fitness:\n",
    "                self.best_fitness = fitness\n",
    "                self.best_position = gazelle.copy()\n",
    "        \n",
    "        fitness_values = np.array(fitness_values)\n",
    "        \n",
    "        # Main optimization loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"GOA Iteration {iteration + 1}/{self.max_iterations}, Best Fitness: {self.best_fitness:.4f}\")\n",
    "            \n",
    "            # Adaptive parameter adjustment\n",
    "            exploration_factor = 1 - (iteration / self.max_iterations)\n",
    "            \n",
    "            for i in range(self.population_size):\n",
    "                # Select random gazelles\n",
    "                candidates = list(range(self.population_size))\n",
    "                candidates.remove(i)\n",
    "                r1, r2 = np.random.choice(candidates, 2, replace=False)\n",
    "                \n",
    "                # Adaptive step size\n",
    "                step_size = (2 * np.random.random() - 1) * exploration_factor\n",
    "                \n",
    "                # Update position with improved movement equations\n",
    "                for j in range(self.dim):\n",
    "                    if np.random.random() < 0.5:\n",
    "                        # Enhanced exploration\n",
    "                        population[i][j] = population[i][j] + step_size * (population[r1][j] - population[r2][j])\n",
    "                    else:\n",
    "                        # Enhanced exploitation with levy flight\n",
    "                        levy_step = self.levy_flight()\n",
    "                        population[i][j] = population[i][j] + step_size * (self.best_position[j] - population[i][j]) + 0.01 * levy_step\n",
    "                    \n",
    "                    # Boundary handling\n",
    "                    lower, upper = bounds[j]\n",
    "                    population[i][j] = np.clip(population[i][j], lower, upper)\n",
    "                \n",
    "                # Evaluate new position\n",
    "                new_fitness = self.fitness_function(population[i], X_train, y_train, X_val, y_val)\n",
    "                \n",
    "                # Update if better\n",
    "                if new_fitness < fitness_values[i]:\n",
    "                    fitness_values[i] = new_fitness\n",
    "                    \n",
    "                    if new_fitness < self.best_fitness:\n",
    "                        self.best_fitness = new_fitness\n",
    "                        self.best_position = population[i].copy()\n",
    "            \n",
    "            self.fitness_history.append(self.best_fitness)\n",
    "            \n",
    "            # Diversity maintenance\n",
    "            if iteration % 10 == 0:\n",
    "                self.maintain_diversity(population, bounds)\n",
    "        \n",
    "        print(f\"Advanced GOA optimization completed. Best fitness: {self.best_fitness:.4f}\")\n",
    "        return self.best_position\n",
    "    \n",
    "    def levy_flight(self):\n",
    "        \"\"\"Generate Levy flight random walk\"\"\"\n",
    "        beta = 3/2\n",
    "        sigma = (np.math.gamma(1 + beta) * np.sin(np.pi * beta / 2) / \n",
    "                (np.math.gamma((1 + beta) / 2) * beta * (2 ** ((beta - 1) / 2)))) ** (1 / beta)\n",
    "        u = np.random.normal(0, sigma)\n",
    "        v = np.random.normal(0, 1)\n",
    "        return u / (abs(v) ** (1 / beta))\n",
    "    \n",
    "    def maintain_diversity(self, population, bounds):\n",
    "        \"\"\"Maintain population diversity\"\"\"\n",
    "        diversity = np.std(population, axis=0)\n",
    "        avg_diversity = np.mean(diversity)\n",
    "        \n",
    "        if avg_diversity < self.diversity_threshold:\n",
    "            # Reinitialize some random individuals\n",
    "            num_reinit = self.population_size // 4\n",
    "            indices = np.random.choice(self.population_size, num_reinit, replace=False)\n",
    "            \n",
    "            for idx in indices:\n",
    "                for j in range(self.dim):\n",
    "                    lower, upper = bounds[j]\n",
    "                    population[idx][j] = np.random.uniform(lower, upper)\n",
    "\n",
    "class EnhancedCICIoT2023MLAlgorithm:\n",
    "    \"\"\"\n",
    "    Enhanced ML algorithms with improved data balancing and feature engineering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()  # More robust to outliers\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.rf_model = None\n",
    "        self.xgb_model = None\n",
    "        self.lgb_model = None\n",
    "        self.best_params = None\n",
    "        self.results = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self, file_path):\n",
    "        \"\"\"Enhanced data loading and preprocessing\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "            \n",
    "            # Identify target column\n",
    "            if 'label' in df.columns:\n",
    "                target_col = 'label'\n",
    "            elif 'Label' in df.columns:\n",
    "                target_col = 'Label'\n",
    "            else:\n",
    "                target_col = df.columns[-1]\n",
    "            \n",
    "            print(f\"Target column: {target_col}\")\n",
    "            print(f\"Target distribution: {df[target_col].value_counts()}\")\n",
    "            \n",
    "            # Move target column to the end\n",
    "            if target_col != df.columns[-1]:\n",
    "                cols = [col for col in df.columns if col != target_col] + [target_col]\n",
    "                df = df[cols]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        # Enhanced preprocessing\n",
    "        df = self.advanced_preprocessing(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def advanced_preprocessing(self, df):\n",
    "        \"\"\"Advanced preprocessing with feature engineering\"\"\"\n",
    "        print(\"Advanced preprocessing...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        # Remove exact duplicates\n",
    "        initial_shape = df.shape\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"Removed {initial_shape[0] - df.shape[0]} duplicate rows\")\n",
    "        \n",
    "        # Handle missing values more intelligently\n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                if column == target_col:\n",
    "                    df = df.dropna(subset=[column])\n",
    "                elif df[column].dtype in ['float64', 'int64']:\n",
    "                    # Use median for numerical columns\n",
    "                    df[column].fillna(df[column].median(), inplace=True)\n",
    "                else:\n",
    "                    # Use mode for categorical columns\n",
    "                    mode_val = df[column].mode()\n",
    "                    if len(mode_val) > 0:\n",
    "                        df[column].fillna(mode_val[0], inplace=True)\n",
    "        \n",
    "        # Advanced outlier handling using IQR method\n",
    "        numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        numerical_columns = [col for col in numerical_columns if col != target_col]\n",
    "        \n",
    "        for column in numerical_columns:\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR > 0:\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Cap extreme outliers\n",
    "                df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "                df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "        \n",
    "        # Feature engineering - create new features\n",
    "        if len(numerical_columns) > 1:\n",
    "            # Create interaction features for top correlated features\n",
    "            feature_corr = df[numerical_columns].corr().abs()\n",
    "            \n",
    "            # Find top correlated pairs\n",
    "            high_corr_pairs = []\n",
    "            for i in range(len(feature_corr.columns)):\n",
    "                for j in range(i+1, len(feature_corr.columns)):\n",
    "                    if 0.3 < feature_corr.iloc[i, j] < 0.9:  # Avoid perfect correlation\n",
    "                        high_corr_pairs.append((feature_corr.columns[i], feature_corr.columns[j]))\n",
    "            \n",
    "            # Create interaction features for top 5 pairs\n",
    "            for i, (col1, col2) in enumerate(high_corr_pairs[:5]):\n",
    "                df[f'interaction_{i}'] = df[col1] * df[col2]\n",
    "                df[f'ratio_{i}'] = df[col1] / (df[col2] + 1e-8)\n",
    "        \n",
    "        print(f\"Final dataset shape after advanced preprocessing: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    def enhanced_class_balancing(self, df, representation_type='2-class'):\n",
    "        \"\"\"Enhanced class balancing with proper handling of all classes\"\"\"\n",
    "        print(f\"Enhanced class balancing for {representation_type}...\")\n",
    "        \n",
    "        target_col = df.columns[-1]\n",
    "        unique_classes = df[target_col].unique()\n",
    "        \n",
    "        if representation_type == '2-class':\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Identify normal class more robustly\n",
    "            normal_keywords = ['normal', 'benign', 'legitimate', 'clean']\n",
    "            normal_class = None\n",
    "            \n",
    "            for class_name in unique_classes:\n",
    "                if any(keyword in str(class_name).lower() for keyword in normal_keywords):\n",
    "                    normal_class = class_name\n",
    "                    break\n",
    "            \n",
    "            if normal_class is None:\n",
    "                # Use the most frequent class as normal\n",
    "                class_counts = df[target_col].value_counts()\n",
    "                normal_class = class_counts.index[0]\n",
    "            \n",
    "            # Create binary classification\n",
    "            df_copy[target_col] = df_copy[target_col].apply(\n",
    "                lambda x: 'Normal' if x == normal_class else 'Attack'\n",
    "            )\n",
    "            \n",
    "            # Balance with 20,000 samples per class for better performance\n",
    "            target_samples = 20000\n",
    "            df_balanced = self.balance_binary_classes(df_copy, target_samples)\n",
    "            \n",
    "        elif representation_type == '8-class':\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Enhanced 8-class mapping\n",
    "            class_mapping = self.create_8_class_mapping(unique_classes)\n",
    "            \n",
    "            # Apply mapping\n",
    "            for new_class, old_classes in class_mapping.items():\n",
    "                df_copy[target_col] = df_copy[target_col].replace(old_classes, new_class)\n",
    "            \n",
    "            # Balance all 8 classes properly\n",
    "            target_samples = 15000  # Increased for better performance\n",
    "            df_balanced = self.balance_multiclass_advanced(df_copy, target_samples)\n",
    "            \n",
    "        elif representation_type == '34-class':\n",
    "            df_copy = df.copy()\n",
    "            \n",
    "            # Keep all original classes but balance them properly\n",
    "            target_samples = 5000  # Reasonable sample size for 34 classes\n",
    "            df_balanced = self.balance_multiclass_advanced(df_copy, target_samples)\n",
    "        \n",
    "        print(f\"Final balanced dataset shape: {df_balanced.shape}\")\n",
    "        print(f\"Class distribution after balancing:\")\n",
    "        print(df_balanced[target_col].value_counts().sort_index())\n",
    "        \n",
    "        return df_balanced\n",
    "    \n",
    "    def create_8_class_mapping(self, unique_classes):\n",
    "        \"\"\"Create intelligent 8-class mapping\"\"\"\n",
    "        class_mapping = {\n",
    "            'Normal': [],\n",
    "            'DDoS': [],\n",
    "            'DoS': [],\n",
    "            'Reconnaissance': [],\n",
    "            'Web_Attack': [],\n",
    "            'Brute_Force': [],\n",
    "            'Spoofing': [],\n",
    "            'Botnet': []\n",
    "        }\n",
    "        \n",
    "        # Find normal class\n",
    "        normal_keywords = ['normal', 'benign', 'legitimate', 'clean']\n",
    "        for class_name in unique_classes:\n",
    "            class_lower = str(class_name).lower()\n",
    "            \n",
    "            if any(keyword in class_lower for keyword in normal_keywords):\n",
    "                class_mapping['Normal'].append(class_name)\n",
    "            elif 'ddos' in class_lower:\n",
    "                class_mapping['DDoS'].append(class_name)\n",
    "            elif 'dos' in class_lower and 'ddos' not in class_lower:\n",
    "                class_mapping['DoS'].append(class_name)\n",
    "            elif any(keyword in class_lower for keyword in ['recon', 'scan', 'probe', 'ping']):\n",
    "                class_mapping['Reconnaissance'].append(class_name)\n",
    "            elif any(keyword in class_lower for keyword in ['web', 'sql', 'xss', 'injection']):\n",
    "                class_mapping['Web_Attack'].append(class_name)\n",
    "            elif any(keyword in class_lower for keyword in ['brute', 'force', 'dictionary']):\n",
    "                class_mapping['Brute_Force'].append(class_name)\n",
    "            elif any(keyword in class_lower for keyword in ['spoof', 'mitm', 'arp']):\n",
    "                class_mapping['Spoofing'].append(class_name)\n",
    "            elif any(keyword in class_lower for keyword in ['bot', 'mirai', 'trojan']):\n",
    "                class_mapping['Botnet'].append(class_name)\n",
    "            else:\n",
    "                # Distribute remaining classes\n",
    "                if len(class_mapping['DDoS']) < len(class_mapping['DoS']):\n",
    "                    class_mapping['DDoS'].append(class_name)\n",
    "                else:\n",
    "                    class_mapping['DoS'].append(class_name)\n",
    "        \n",
    "        return class_mapping\n",
    "    \n",
    "    def balance_binary_classes(self, df, target_samples):\n",
    "        \"\"\"Balance binary classes with advanced techniques\"\"\"\n",
    "        target_col = df.columns[-1]\n",
    "        \n",
    "        normal_data = df[df[target_col] == 'Normal']\n",
    "        attack_data = df[df[target_col] == 'Attack']\n",
    "        \n",
    "        # Ensure we have enough samples\n",
    "        if len(normal_data) < target_samples:\n",
    "            normal_data = resample(normal_data, n_samples=target_samples, \n",
    "                                 random_state=42, replace=True)\n",
    "        else:\n",
    "            normal_data = normal_data.sample(n=target_samples, random_state=42)\n",
    "        \n",
    "        if len(attack_data) < target_samples:\n",
    "            attack_data = resample(attack_data, n_samples=target_samples, \n",
    "                                 random_state=42, replace=True)\n",
    "        else:\n",
    "            attack_data = attack_data.sample(n=target_samples, random_state=42)\n",
    "        \n",
    "        return pd.concat([normal_data, attack_data], ignore_index=True)\n",
    "    \n",
    "    def balance_multiclass_advanced(self, df, target_samples):\n",
    "        \"\"\"Advanced multiclass balancing ensuring all classes are properly balanced\"\"\"\n",
    "        target_col = df.columns[-1]\n",
    "        unique_classes = df[target_col].unique()\n",
    "        \n",
    "        print(f\"Balancing {len(unique_classes)} classes to {target_samples} samples each...\")\n",
    "        \n",
    "        balanced_data = []\n",
    "        \n",
    "        for class_label in unique_classes:\n",
    "            class_data = df[df[target_col] == class_label]\n",
    "            current_samples = len(class_data)\n",
    "            \n",
    "            print(f\"Class '{class_label}': {current_samples} -> {target_samples} samples\")\n",
    "            \n",
    "            if current_samples == 0:\n",
    "                print(f\"Warning: No samples found for class '{class_label}', skipping...\")\n",
    "                continue\n",
    "            \n",
    "            if current_samples < target_samples:\n",
    "                # Upsample minority class\n",
    "                upsampled = resample(class_data, n_samples=target_samples, \n",
    "                                   random_state=42, replace=True)\n",
    "                balanced_data.append(upsampled)\n",
    "            elif current_samples > target_samples:\n",
    "                # Downsample majority class\n",
    "                downsampled = class_data.sample(n=target_samples, random_state=42)\n",
    "                balanced_data.append(downsampled)\n",
    "            else:\n",
    "                # Already balanced\n",
    "                balanced_data.append(class_data)\n",
    "        \n",
    "        df_balanced = pd.concat(balanced_data, ignore_index=True)\n",
    "        \n",
    "        # Apply SMOTE for additional balancing\n",
    "        X = df_balanced.drop(target_col, axis=1)\n",
    "        y = df_balanced[target_col]\n",
    "        \n",
    "        try:\n",
    "            # Use SMOTE with adjusted parameters\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(3, target_samples-1))\n",
    "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "            \n",
    "            # Combine back to dataframe\n",
    "            df_final = pd.concat([\n",
    "                pd.DataFrame(X_resampled, columns=X.columns),\n",
    "                pd.Series(y_resampled, name=target_col)\n",
    "            ], axis=1)\n",
    "            \n",
    "            return df_final\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"SMOTE failed: {e}, using basic balancing\")\n",
    "            return df_balanced\n",
    "    \n",
    "    def train_enhanced_hybrid_model(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train enhanced hybrid model with 3 algorithms\"\"\"\n",
    "        print(\"Training enhanced hybrid model (RF + XGB + LightGBM)...\")\n",
    "        \n",
    "        # Encode labels\n",
    "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
    "        y_val_encoded = self.label_encoder.transform(y_val)\n",
    "        \n",
    "        # Enhanced hyperparameter bounds\n",
    "        bounds = [\n",
    "            (100, 300),   # RF n_estimators\n",
    "            (5, 25),      # RF max_depth\n",
    "            (2, 10),      # RF min_samples_split\n",
    "            (1, 5),       # RF min_samples_leaf\n",
    "            (100, 500),   # XGB n_estimators\n",
    "            (3, 15),      # XGB max_depth\n",
    "            (0.01, 0.3),  # XGB learning_rate\n",
    "            (0.6, 1.0),   # XGB subsample\n",
    "            (0.6, 1.0),   # XGB colsample_bytree\n",
    "            (50, 300),    # LGB n_estimators\n",
    "            (0.01, 0.3),  # LGB learning_rate\n",
    "            (0.0, 3.0)    # Weight parameter (will be normalized)\n",
    "        ]\n",
    "        \n",
    "        # Run enhanced GOA\n",
    "        goa = AdvancedGazelleOptimizationAlgorithm(\n",
    "            population_size=50, max_iterations=150, dim=12\n",
    "        )\n",
    "        self.best_params = goa.optimize(X_train, y_train_encoded, X_val, y_val_encoded, bounds)\n",
    "        \n",
    "        # Extract optimized parameters\n",
    "        rf_params = {\n",
    "            'n_estimators': int(self.best_params[0]),\n",
    "            'max_depth': int(self.best_params[1]),\n",
    "            'min_samples_split': int(self.best_params[2]),\n",
    "            'min_samples_leaf': int(self.best_params[3]),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "        \n",
    "        xgb_params = {\n",
    "            'n_estimators': int(self.best_params[4]),\n",
    "            'max_depth': int(self.best_params[5]),\n",
    "            'learning_rate': self.best_params[6],\n",
    "            'subsample': self.best_params[7],\n",
    "            'colsample_bytree': self.best_params[8],\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        lgb_params = {\n",
    "            'n_estimators': int(self.best_params[9]),\n",
    "            'learning_rate': self.best_params[10],\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1,\n",
    "            'class_weight': 'balanced',\n",
    "            'verbosity': -1\n",
    "        }\n",
    "        \n",
    "        # Normalize ensemble weights\n",
    "        total_weight = self.best_params[11]\n",
    "        self.rf_weight = total_weight / 3\n",
    "        self.xgb_weight = total_weight / 3\n",
    "        self.lgb_weight = 1 - self.rf_weight - self.xgb_weight\n",
    "        \n",
    "        print(f\"Optimized parameters:\")\n",
    "        print(f\"RF: {rf_params}\")\n",
    "        print(f\"XGB: {xgb_params}\")\n",
    "        print(f\"LGB: {lgb_params}\")\n",
    "        print(f\"Weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}, LGB: {self.lgb_weight:.3f}\")\n",
    "        \n",
    "        # Train models\n",
    "        self.rf_model = RandomForestClassifier(**rf_params)\n",
    "        self.xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "        self.lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "        \n",
    "        self.rf_model.fit(X_train, y_train_encoded)\n",
    "        self.xgb_model.fit(X_train, y_train_encoded)\n",
    "        self.lgb_model.fit(X_train, y_train_encoded)\n",
    "        \n",
    "        # Store feature importance\n",
    "        self.feature_importance = {\n",
    "            'rf': self.rf_model.feature_importances_,\n",
    "            'xgb': self.xgb_model.feature_importances_,\n",
    "            'lgb': self.lgb_model.feature_importances_\n",
    "        }\n",
    "        \n",
    "        print(\"Enhanced hybrid model training completed!\")\n",
    "    \n",
    "    def predict_enhanced_hybrid(self, X_test):\n",
    "        \"\"\"Make predictions using enhanced hybrid model\"\"\"\n",
    "        rf_pred_proba = self.rf_model.predict_proba(X_test)\n",
    "        xgb_pred_proba = self.xgb_model.predict_proba(X_test)\n",
    "        lgb_pred_proba = self.lgb_model.predict_proba(X_test)\n",
    "        \n",
    "        # Weighted ensemble prediction\n",
    "        ensemble_pred_proba = (self.rf_weight * rf_pred_proba + \n",
    "                              self.xgb_weight * xgb_pred_proba + \n",
    "                              self.lgb_weight * lgb_pred_proba)\n",
    "        \n",
    "        ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
    "        \n",
    "        return self.label_encoder.inverse_transform(ensemble_pred)\n",
    "    \n",
    "    def evaluate_enhanced_model(self, X_test, y_test):\n",
    "        \"\"\"Enhanced model evaluation\"\"\"\n",
    "        print(\"Evaluating enhanced hybrid model...\")\n",
    "        \n",
    "        y_pred = self.predict_enhanced_hybrid(X_test)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        per_class_metrics = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        self.results = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Per_Class_Metrics': per_class_metrics\n",
    "        }\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def run_enhanced_algorithm(self, file_path, representation_type='2-class'):\n",
    "        \"\"\"Run the enhanced algorithm\"\"\"\n",
    "        print(f\"Starting Enhanced CICIoT2023 ML Algorithm with {representation_type}...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        df = self.load_and_preprocess_data(file_path)\n",
    "        \n",
    "        # Enhanced class balancing\n",
    "        df_balanced = self.enhanced_class_balancing(df, representation_type)\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df_balanced.iloc[:, :-1].values\n",
    "        y = df_balanced.iloc[:, -1].values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        X_train_opt, X_val, y_train_opt, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {X_train_opt.shape[0]}\")\n",
    "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Step 5: Normalize features\n",
    "        X_train_scaled, X_test_scaled = self.normalize_features(X_train_opt, X_test)\n",
    "        X_val_scaled = self.scaler.transform(X_val)\n",
    "        \n",
    "        # Step 6: Train hybrid model with GOA optimization\n",
    "        self.train_hybrid_model(X_train_scaled, y_train_opt, X_val_scaled, y_val)\n",
    "        \n",
    "        # Step 7: Evaluate model\n",
    "        results = self.evaluate_model(X_test_scaled, y_test)\n",
    "        \n",
    "        # Display results\n",
    "        self.display_results(representation_type)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, representation_type):\n",
    "        \"\"\"\n",
    "        Display evaluation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"EVALUATION RESULTS - {representation_type.upper()} REPRESENTATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Hybrid Random Forest + XGBoost Model (GOA Optimized):\")\n",
    "        print(f\"  Accuracy:  {self.results['Accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {self.results['Precision']:.4f}\")\n",
    "        print(f\"  Recall:    {self.results['Recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {self.results['F1-Score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nOptimized Hyperparameters:\")\n",
    "        print(f\"  RF n_estimators: {int(self.best_params[0])}\")\n",
    "        print(f\"  RF max_depth: {int(self.best_params[1]) if self.best_params[1] > 0 else None}\")\n",
    "        print(f\"  XGB n_estimators: {int(self.best_params[4])}\")\n",
    "        print(f\"  XGB learning_rate: {self.best_params[6]:.3f}\")\n",
    "        print(f\"  Ensemble weights - RF: {self.rf_weight:.3f}, XGB: {self.xgb_weight:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the algorithm\n",
    "    ml_algorithm = AdvancedGazelleOptimizationAlgorithm()\n",
    "    \n",
    "    print(\"CICIoT2023 Dataset ML Algorithm with GOA and Hybrid RF-XGB\")\n",
    "    print(\"This implementation includes:\")\n",
    "    print(\"- Gazelle Optimization Algorithm (GOA) for hyperparameter optimization\")\n",
    "    print(\"- Hybrid Random Forest + XGBoost ensemble model\")\n",
    "    print(\"- Three class representations: 2-class, 8-class, and 34-class\")\n",
    "    print(\"\\nTo use this algorithm:\")\n",
    "    print(\"1. Ensure you have the CICIoT2023 dataset in CSV format\")\n",
    "    print(\"2. Install required packages: pip install pandas scikit-learn imbalanced-learn xgboost\")\n",
    "    print(\"3. Run for different representations:\")\n",
    "    print(\"   - 2-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '2-class')\")\n",
    "    print(\"   - 8-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '8-class')\")\n",
    "    print(\"   - 34-class: ml_algorithm.run_algorithm('path_to_dataset.csv', '34-class')\")\n",
    "    \n",
    "     # Uncomment the following lines and provide the correct path to run the algorithm\n",
    "    results_2class = ml_algorithm.run_enhanced_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '2-class')\n",
    "    results_8class = ml_algorithm.run_enhanced_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '8-class')\n",
    "    #results_34class = ml_algorithm.run_algorithm('C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\capstone\\\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv', '34-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c06ea-e2a5-4958-82eb-f67cc6e8d45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcb0ec-f873-41f8-b3ce-f469bceaf9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
